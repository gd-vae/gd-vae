{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GD-VAE: Constrained Mechanics\n",
    "[http://atzberger.org](http://atzberger.org)\n",
    "\n",
    "#### Overview\n",
    "\n",
    "We develop variational autoencoders for dimension reduction of data from a constrained mechanical system. This trains models for embedding data within a manifold latent space with prescribed topology and geometry. The manifold is specified by a point cloud representation allowing for flexibility in the specified geometry. When analytic expressions are known these also can be used. For more information see the GD-VAEs paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arm Mechanics\n",
    "\n",
    "Consider a robotics arm consisting of two rod segments in a 2D space. The arm configuration are given by the locations $x_1, x_2 \\in \\mathbb{R}^2$.  In the rigid case the state of the system could in principle be represented by two angles $\\theta_1$ and $\\theta_2$ in $[0,2\\pi]$. However, using angles as latent representations can be problematic in practice since the angle variables can jump discontinuously between $0$ and $2\\pi$ when representing configurations. We instead use a 2Dx2D latent structure in $\\mathbb{R}^4$. We can map this to $2$ Degrees of Freedom (DOF) while avoiding coordinate singularities by using a manifold latent space.\n",
    "\n",
    "In the case of rigid rods, the periodicity results in the configurations mapping naturally to a torus manifold. More exotic mechanical constraints can also be considered such as configurations of the arm lengths resulting in points $x_1$ and $x_2$ being on a klein bottle.  For a figure illustrating the arm geometry see the examples folder or paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,os,sys,pickle,time,shutil,logging,numpy as np,matplotlib,matplotlib.pyplot as plt;\n",
    "import gd_vae_pytorch as gd_vae,gd_vae_pytorch.geo_map,gd_vae_pytorch.vae;\n",
    "import gd_vae_pytorch.nn,gd_vae_pytorch.utils; \n",
    "\n",
    "# local packages\n",
    "sys.path.insert(1,'.');\n",
    "import pkg,pkg.datasets,pkg.vis,pkg.geometry,pkg.model_utils;\n",
    "\n",
    "# script name without extension\n",
    "script_base_name = 'arm_mechanics1'; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the parameters\n",
    "batch_size = int(1e3); # int(1e2);\n",
    "flag_save_fig = True;\n",
    "\n",
    "# select dataset\n",
    "#flag_dataset = 'arm1_torus';\n",
    "flag_dataset = 'arm1_klein';\n",
    "if flag_dataset == 'arm1_torus':  \n",
    "  num_dim_x = 4;   \n",
    "\n",
    "  # select model\n",
    "  flag_model = 'gd_vae_nn1_torus';\n",
    "  #flag_model = 'gd_vae_nn1_R_n';\n",
    "  if flag_model == 'gd_vae_nn1_torus':\n",
    "    num_dim_z = 4; \n",
    "  else:\n",
    "    num_dim_z = 10; # R^n case any value fine\n",
    "\n",
    "elif flag_dataset == 'arm1_klein':\n",
    "  num_dim_x = 4;      \n",
    "\n",
    "  # select model\n",
    "  flag_model = 'gd_vae_nn1_klein';\n",
    "  #flag_model = 'gd_vae_nn1_R_n';  \n",
    "  if flag_model == 'gd_vae_nn1_klein':\n",
    "    num_dim_z = 4; \n",
    "  else:\n",
    "    num_dim_z = 10; # R^n case any value fine\n",
    "\n",
    "# run name and output directory \n",
    "run_name = '%s__%s__test_0001'%(flag_dataset,flag_model);\n",
    "base_dir = './output/%s/%s'%(script_base_name,run_name);\n",
    "\n",
    "# create directory for output (removes previous data)\n",
    "gd_vae.utils.rm_dir(base_dir); gd_vae.utils.create_dir(base_dir);\n",
    "\n",
    "base_dir_training = '%s/training'%(base_dir);\n",
    "gd_vae.utils.create_dir(base_dir_training);\n",
    "\n",
    "# setup logging\n",
    "#gd_vae.log.setup_log(base_dir);\n",
    "\n",
    "# display some information about this study\n",
    "print(\"script_base_name = \" + script_base_name);\n",
    "print(\"\"); print(\"flag_dataset = \" + flag_dataset); print(\"flag_model = \" + flag_model); \n",
    "print(\"run_name = \" + run_name); print(\"base_dir = \" + base_dir); \n",
    "\n",
    "# model parameters\n",
    "total_N_xi      = int(1e6); \n",
    "num_dim_x_tilde = 2; n_tau = 2;\n",
    "num_dim_x       = num_dim_x_tilde*n_tau;\n",
    "\n",
    "print(\"\"); print(\"num_dim_x = \" + str(num_dim_x));\n",
    "print(\"num_dim_z = \" + str(num_dim_z));\n",
    "\n",
    "# display system information\n",
    "print(\"\"); print(\"torch.__version__ = \" + str(torch.__version__));\n",
    "print(\"numpy.__version__ = \" + str(np.__version__));\n",
    "print(\"gd_vae.vae.__version__ = \" + str(gd_vae.vae.__version__));\n",
    "\n",
    "# configure the device(s) to use\n",
    "if torch.cuda.is_available():  \n",
    "  num_gpus = torch.cuda.device_count();\n",
    "  print(\"\");  print(\"num_gpus = \" + str(num_gpus));\n",
    "  if num_gpus >= 4:\n",
    "    device = torch.device('cuda:2');\n",
    "  else:\n",
    "    device = torch.device('cuda:0');\n",
    "else: \n",
    "  device = torch.device('cpu');\n",
    "print(\"device = \" + str(device));\n",
    "\n",
    "# start saving information about this study\n",
    "s_data = {}; tensor_names = {};\n",
    "s_data.update({'comment':\"\",'torch.__version__':str(torch.__version__),\n",
    "               'numpy.__version__':str(np.__version__),\n",
    "               'gd_vae.vae.__version__':str(gd_vae.vae.__version__),\n",
    "               'num_dim_x':num_dim_x,'num_dim_x_tilde':num_dim_x_tilde,\n",
    "               'total_N_xi':total_N_xi,'n_tau':n_tau,\n",
    "               'device':str(device),'base_dir':base_dir,'run_name':run_name,\n",
    "               'flag_dataset':flag_dataset,'flag_model':flag_model,\n",
    "               'num_dim_x':num_dim_x,'num_dim_z':num_dim_z});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_dataset == 'arm1_torus':\n",
    "    flag_model_type = 'arm1';\n",
    "    dgen_type = 'arm1_torus'; s_data['dgen_type'] = dgen_type;\n",
    "\n",
    "    params_gen = {};\n",
    "    num_samples = int(1e4); num_theta = 2;    \n",
    "    params_gen = {'num_samples':num_samples,'num_dim':2,'num_theta':num_theta,'device':device};\n",
    "    params_gen.update({'noise_type':'Gaussian1','params_noise':{'sigma':7.5e-2},'sample_mode':'with_noise'});\n",
    "    ell_list = torch.ones(num_samples,num_theta,device=device); ell_list[:,0] = 1.0; ell_list[:,1] = 0.5;\n",
    "    params_gen.update({'ell_list':ell_list});\n",
    "\n",
    "    # create dataset and loaders \n",
    "    train_dataset = pkg.datasets.GenDataArm1Rigid(**params_gen,flag_verbose=0).to(device);\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True);\n",
    "\n",
    "    test_dataset = pkg.datasets.GenDataArm1Rigid(**params_gen,flag_verbose=0).to(device);\n",
    "    test_loader  = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False);\n",
    "\n",
    "    # plot\n",
    "    pkg.vis.plot_arm1_data(train_dataset,base_dir);\n",
    "    \n",
    "elif flag_dataset == 'arm1_klein':\n",
    "    flag_model_type = 'arm1';\n",
    "    dgen_type = 'arm1_klein'; s_data['dgen_type'] = dgen_type;\n",
    "    \n",
    "    params_gen = {}; num_samples = int(3e3);\n",
    "    params_klein = {'a':1.6,'b':1,'n1':200,'n2':200,'device':device};\n",
    "    params_gen.update({'num_samples':num_samples,'num_dim':2,'num_dim_X':4,'params_klein':params_klein,'device':device});\n",
    "    params_gen.update({'noise_type':'Gaussian1','params_noise':{'sigma':7.5e-2},'sample_mode':'with_noise'});    \n",
    "        \n",
    "    # create dataset and loaders\n",
    "    train_dataset = pkg.datasets.GenDataArm1Klein(**params_gen,flag_verbose=0).to(device);\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True);\n",
    "\n",
    "    test_dataset = pkg.datasets.GenDataArm1Klein(**params_gen,flag_verbose=0).to(device);\n",
    "    test_loader  = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False);\n",
    "\n",
    "    # plot\n",
    "    pkg.vis.plot_arm1_data(train_dataset,base_dir);      \n",
    "\n",
    "else:\n",
    "    raise Exception(\"Not recognized, flag_dataset = \" + str(flag_dataset));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Model and Neural Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_params = {};\n",
    "    \n",
    "if flag_model == 'gd_vae_nn1_R_n':\n",
    "  #num_dim_z = 4; # uses value from above\n",
    "\n",
    "  manifold_map = None; # no manifold map, just use R^n latent space\n",
    "\n",
    "  # -- theta (decoder)\n",
    "  input_size = num_dim_z; output_size = num_dim_x;\n",
    "  layer_sizes = [input_size,int(1e2),int(5e2),int(1e2),output_size]; layer_biases = [True,True,True,False];\n",
    "  params_theta = {'layer_sizes':layer_sizes,'layer_biases':layer_biases,'manifold_map':manifold_map};\n",
    "\n",
    "  # -- phi (encoder)\n",
    "  input_size = num_dim_x; output_size = num_dim_z;\n",
    "  layer_sizes = [input_size,int(1e2),int(5e2),int(1e2),output_size]; layer_biases = [True,True,True,False];\n",
    "  params_phi = {'layer_sizes':layer_sizes,'layer_biases':layer_biases,'manifold_map':manifold_map};\n",
    "  \n",
    "  # -- general parameters\n",
    "  type_model = 'gd_vae_nn1';  \n",
    "  params_create = {}; params_create.update({'type_model':type_model,'params_theta':params_theta,\n",
    "                                             'params_phi':params_phi,'device':device});\n",
    "  model = pkg.model_utils.create_model(**params_create);\n",
    "    \n",
    "  theta = model['theta']; phi = model['phi'];\n",
    "  \n",
    "  print(\"theta (decoder):\"); # decoder  \n",
    "  print(\"manifold_map = \" + str(params_theta['manifold_map']));\n",
    "  print(\"model_mu = \" + str(theta['model_mu']));\n",
    "  print(\"model_log_sigma_sq = \" + str(theta['model_log_sigma_sq']));\n",
    "\n",
    "  print(\"phi (encoder):\"); # decoder\n",
    "  print(\"manifold_map = \" + str(params_phi['manifold_map']));\n",
    "  print(\"model_mu = \" + str(phi['model_mu']));\n",
    "  print(\"model_log_sigma_sq = \" + str(phi['model_log_sigma_sq']));  \n",
    "\n",
    "elif flag_model == 'gd_vae_nn1_torus':\n",
    "  #num_dim_z = 4; # uses value from above\n",
    "\n",
    "  # -- create manifold map to torus\n",
    "  manifold_map = pkg.model_utils.create_manifold_map_torus(device=device);\n",
    "  \n",
    "  # -- theta (decoder)\n",
    "  input_size = num_dim_z; output_size = num_dim_x;\n",
    "  layer_sizes = [input_size,int(1e2),int(5e2),int(1e2),output_size]; layer_biases = [True,True,True,False];\n",
    "  params_theta = {'layer_sizes':layer_sizes,'layer_biases':layer_biases,'manifold_map':manifold_map};\n",
    "\n",
    "  # -- phi (encoder)\n",
    "  input_size = num_dim_x; output_size = num_dim_z;\n",
    "  layer_sizes = [input_size,int(1e2),int(5e2),int(1e2),output_size]; layer_biases = [True,True,True,False];\n",
    "  params_phi = {'layer_sizes':layer_sizes,'layer_biases':layer_biases,'manifold_map':manifold_map};\n",
    "  \n",
    "  # -- general parameters\n",
    "  type_model = 'gd_vae_nn1';      \n",
    "  params_create = {}; params_create.update({'type_model':type_model,'params_theta':params_theta,\n",
    "                                             'params_phi':params_phi,'device':device});\n",
    "  model = pkg.model_utils.create_model(**params_create);\n",
    "    \n",
    "  theta = model['theta']; phi = model['phi'];\n",
    "  \n",
    "  print(\"theta (decoder):\"); # decoder  \n",
    "  print(\"manifold_map = \" + str(params_theta['manifold_map']));\n",
    "  print(\"model_mu = \" + str(theta['model_mu']));\n",
    "  print(\"model_log_sigma_sq = \" + str(theta['model_log_sigma_sq']));\n",
    "\n",
    "  print(\"phi (encoder):\"); # decoder\n",
    "  print(\"manifold_map = \" + str(params_phi['manifold_map']));\n",
    "  print(\"model_mu = \" + str(phi['model_mu']));\n",
    "  print(\"model_log_sigma_sq = \" + str(phi['model_log_sigma_sq']));  \n",
    "\n",
    "elif flag_model == 'gd_vae_nn1_klein':\n",
    "\n",
    "  #num_dim_z = 4; # uses value from above\n",
    "\n",
    "  # -- create manifold map to klein bottle \n",
    "  params_klein = {'a':1.6,'b':1,'n1':200,'n2':200,'device':device};\n",
    "  manifold_map = pkg.model_utils.create_manifold_map_klein1(params_klein=params_klein,device=device);\n",
    "\n",
    "  # -- theta (decoder)\n",
    "  input_size = num_dim_z; output_size = num_dim_x;\n",
    "  layer_sizes = [input_size,int(1e2),int(5e2),int(1e2),output_size]; layer_biases = [True,True,True,False];\n",
    "  params_theta = {'layer_sizes':layer_sizes,'layer_biases':layer_biases,'manifold_map':manifold_map};  \n",
    "  \n",
    "  # -- phi (encoder)\n",
    "  input_size = num_dim_x; output_size = num_dim_z;\n",
    "  layer_sizes = [input_size,int(1e2),int(5e2),int(1e2),output_size]; layer_biases = [True,True,True,False];\n",
    "  params_phi = {'layer_sizes':layer_sizes,'layer_biases':layer_biases,'manifold_map':manifold_map};\n",
    "  \n",
    "  # -- general parameters\n",
    "  type_model = 'gd_vae_nn1';      \n",
    "  params_create = {}; params_create.update({'type_model':type_model,'params_theta':params_theta,\n",
    "                                            'params_phi':params_phi,'device':device});\n",
    "  model = pkg.model_utils.create_model(**params_create);\n",
    "    \n",
    "  theta = model['theta']; phi = model['phi'];\n",
    "  \n",
    "  print(\"theta (decoder):\"); # decoder    \n",
    "  print(\"model_mu = \" + str(theta['model_mu']));\n",
    "  print(\"model_log_sigma_sq = \" + str(theta['model_log_sigma_sq']));\n",
    "\n",
    "  print(\"phi (encoder):\"); # decoder  \n",
    "  print(\"model_mu = \" + str(phi['model_mu']));\n",
    "  print(\"model_log_sigma_sq = \" + str(phi['model_log_sigma_sq']));    \n",
    "else:\n",
    "  raise Exception(\"Not recognized, flag_model = \" + flag_model);\n",
    "\n",
    "# save the model specification\n",
    "model_create_filename = '%s/model_params.pickle'%base_dir;\n",
    "print(\"model_create_filename = \" + model_create_filename);\n",
    "fid = open(model_create_filename,'wb'); pickle.dump(params_create,fid); fid.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup additional parameters\n",
    "extra_params.update({'m1_mc':100,'batch_size':batch_size,'total_N_xi':total_N_xi});\n",
    "extra_params.update({'num_dim_x':num_dim_x,'num_dim_x_tilde':num_dim_x_tilde,'num_dim_z':num_dim_z});\n",
    "extra_params.update({'n_tau':n_tau,'device':device,'return_extra_vals':{}}); # dictionary for returning extra values\n",
    "\n",
    "print(\"extra_params:\\n\" + str(extra_params)); print(\"\");\n",
    "print(\"theta:\\n\" + str(theta)); print(\"\"); print(\"phi:\\n\" + str(phi));\n",
    "\n",
    "pkg.model_utils.init_model_arm1_pretrain_decoder1_init(theta,phi);\n",
    "pkg.model_utils.save_model(theta,phi,type_model,'%s/model'%base_dir_training,'init');  \n",
    "\n",
    "s_data.update({'flag_model_type':flag_model_type,'total_N_xi':total_N_xi,\n",
    "               'num_dim_x':num_dim_x,'num_dim_z':num_dim_z,\n",
    "               'extra_params':extra_params});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display initial configuration and model prediction\n",
    "X_batch = next(iter(train_loader)); # get first dataset batch  \n",
    "fig_list, fig_names = pkg.vis.plot_model(theta,phi,X_batch,flag_model_type,\n",
    "                                         flag_legends=True);\n",
    "\n",
    "for (fig,fig_name) in zip(fig_list,fig_names):\n",
    "  base_filename = '%s/initial_model_%s'%(base_dir,fig_name);\n",
    "  gd_vae.utils.save_fig(base_filename,fig=fig,flag_pdf=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_run_cell = True;\n",
    "if flag_run_cell:\n",
    "    model_log_sigma_sq = theta['model_log_sigma_sq'];\n",
    "    p = model_log_sigma_sq[0]\n",
    "\n",
    "    num_samples = 10; num_dim_x = 4;\n",
    "    X = torch.zeros(num_samples,num_dim_x,device=device);\n",
    "    X = X.requires_grad_(True);\n",
    "    Y = p(X)\n",
    "\n",
    "    loss = torch.sum(Y);\n",
    "\n",
    "    loss.backward();\n",
    "    \n",
    "    print(\"Y.shape = \" + str(Y.shape));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train GD-VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup training parameters\n",
    "num_epochs = int(1e6); learning_rate = 1e-4;\n",
    "#num_epochs = int(1e4); learning_rate = 1e-1;\n",
    "flag_train_variance = False; flag_track_inter_terms = False; flag_track_weights_select = False;\n",
    "flag_save_training_model = True; skip_save_training_model = int(1e3); flag_save_networks = True;\n",
    "skip_save_network = int(1e2); flag_save_model_errors = True; skip_save_model_errors = int(1e2);\n",
    "\n",
    "loss_list = np.empty(0); loss_step_list = np.empty(0); save_skip = 1; epoch = 0; display_skip = int(1e2);\n",
    "\n",
    "extra_params['batch_size'] = batch_size;\n",
    "\n",
    "s_data = {'num_epochs':num_epochs,'learning_rate':learning_rate,'flag_train_variance':flag_train_variance,\n",
    "          'flag_track_inter_terms':flag_track_inter_terms,'flag_track_weights_select':flag_track_weights_select,          \n",
    "          'extra_params':extra_params};\n",
    "\n",
    "# save the parameter data for the run\n",
    "print(\"Saving parameters\"); print(\"\");\n",
    "filename_save_data = '%s/parameters.pickle'%base_dir;\n",
    "print(\"filename_save_data = \" + filename_save_data);\n",
    "pickle.dump(s_data,open(filename_save_data,'wb'));\n",
    "\n",
    "params_test_model_errors = {'base_dir_training':base_dir_training,\n",
    "                            'ref_dataset':test_dataset,'device':device};\n",
    "\n",
    "print(\"\"); print(\"Training the network with:\"); print(\"\");\n",
    "\n",
    "# setup the optimization method and loss function\n",
    "# define the parameters over which we are optimizing\n",
    "model_param_select = gd_vae.utils.DictAsMembers(pkg.model_utils.get_data_of_model_select(theta,phi,flag_model_type));\n",
    "\n",
    "params_to_opt = [];\n",
    "params_to_opt += list(phi['model_mu'].parameters()); # assumes is sequential, train the non-filter layer, mlp 1 assumed only part.\n",
    "params_to_opt += list(theta['model_mu'].parameters()); \n",
    "print(\"theta['model_mu'] = \" + str(theta['model_mu']));\n",
    "print(\"phi['model_mu'] = \" + str(phi['model_mu']));\n",
    "if flag_train_variance:\n",
    "  mm = phi['model_mu']; # note assumed Sequential type with filter layer first\n",
    "  params_to_opt += list(phi['model_log_sigma_sq'].mlp.parameters()); # assumes is sequential, train the non-filter layer, mlp 1 assumed only part.\n",
    "  params_to_opt += list(theta['model_log_sigma_sq'].parameters());  \n",
    "  params_to_opt += list(theta['nu']); # not clear this is changed\n",
    "\n",
    "optimizer = torch.optim.Adam(params_to_opt, lr=learning_rate);\n",
    "print(\"optimizer = \" + str(optimizer));    \n",
    "    \n",
    "# setup loss function\n",
    "loss_func = gd_vae.vae.loss_VAE_neg_tilde_L_B;\n",
    "print(\"loss_func = \" + str(loss_func));\n",
    "print(\"extra_params.keys() = \" + str(extra_params.keys()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"num_epochs = %d\"%num_epochs); print(\"batch_size = %d\"%batch_size); print(\" \");\n",
    "\n",
    "# train the model\n",
    "flag_print_params = True; print(\"-\"*80); num_steps = len(train_loader);\n",
    "flag_quit = False; step_count = 0;\n",
    "while flag_quit is False:\n",
    "    for i, X_batch in enumerate(train_loader):\n",
    "        \n",
    "        # compute loss\n",
    "        loss = loss_func(X_batch,theta,phi,**extra_params);\n",
    "\n",
    "        # display\n",
    "        if step_count % save_skip == 0:\n",
    "          np_loss   = loss.cpu().detach().numpy();\n",
    "          loss_list = np.append(loss_list,np_loss);\n",
    "          loss_step_list = np.append(loss_step_list,step_count);\n",
    "\n",
    "        # back-propagation for gradients and use to optimize\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step();\n",
    "        \n",
    "        # display and plot results of the step\n",
    "        if (step_count % display_skip) == 0:\n",
    "          msg = 'epoch: [%d/%d]; '%(epoch+1,num_epochs);\n",
    "          msg += 'batch_step = [%d/%d]; '%(i + 1,num_steps);\n",
    "          msg += 'loss: %.3e.'%(loss.item());\n",
    "          print(msg);\n",
    "\n",
    "          if flag_print_params:            \n",
    "            pkg.model_utils.print_params_select(theta,phi,flag_model_type);\n",
    "          \n",
    "          if flag_print_params:            \n",
    "            print(\".\"*80); \n",
    "\n",
    "        # save information related to the training\n",
    "        if flag_track_inter_terms: # track terms in loss function for learning dynamics\n",
    "          D_KL_q_phi_z_x__p_theta_z = extra_params['return_extra_vals']['D_KL_q_phi_z_x__p_theta_z'];\n",
    "          avg_sum_log_p_theta__x__z = extra_params['return_extra_vals']['avg_sum_log_p_theta__x__z']; \n",
    "                    \n",
    "        if flag_save_training_model and (step_count % skip_save_training_model == 0):\n",
    "          filename = '%s/training_model_select_%.8d.pickle'%(base_dir_training,step_count);\n",
    "          print(\"filename = \" + filename);\n",
    "          model_param_select = pkg.model_utils.get_data_of_model_select(theta,phi,flag_model_type);\n",
    "          f = open(filename,'wb'); pickle.dump(model_param_select,f); f.close();\n",
    "            \n",
    "        if flag_save_model_errors and (step_count % skip_save_model_errors == 0):               \n",
    "          print(\"Computing model errors.\");\n",
    "          params_test_model_errors.update({'step_count':step_count,'sigma_list':[1e-2,5e-2,1e-1,5e-1]});\n",
    "          pkg.model_utils.test_model_errors(theta,phi,**params_test_model_errors);\n",
    "                                            \n",
    "        if flag_save_networks  and (step_count % skip_save_network == 0):\n",
    "          pkg.model_utils.save_model(theta,phi,type_model,'%s/model'%base_dir_training,'%.7d'%step_count);  \n",
    "        \n",
    "        # update count of training steps\n",
    "        step_count += 1; \n",
    "        if step_count > num_epochs:\n",
    "          flag_quit = True;\n",
    "\n",
    "print(\"done training.\"); print(\"-\"*80);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Prediction of the Learned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show prediction of the trained model\n",
    "flag_run_cell = True;\n",
    "if flag_run_cell:  \n",
    "  input = next(iter(train_loader)); # get first dataset batch  \n",
    "\n",
    "  fig_list, fig_names = pkg.vis.plot_model(theta,phi,input,flag_model_type,flag_legends=True);\n",
    "  \n",
    "  for (fig,fig_name) in zip(fig_list,fig_names):\n",
    "    base_filename = '%s/learned_model_%s'%(base_dir,fig_name);\n",
    "    gd_vae.utils.save_fig(base_filename,fig=fig);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch37_1_10_1]",
   "language": "python",
   "name": "conda-env-pytorch37_1_10_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
