@article{Hinton2006,
  author =        {Geoffrey Hinton and Ruslan Salakhutdinov},
  journal =       {Science},
  number =        {5786},
  pages =         {504-507},
  title =         {Reducing the Dimensionality of Data with Neural
                   Networks},
  volume =        {313},
  year =          {2006},
  doi =           {10.1126/science.1127647},
  timestamp =     {2020.09.11},
  url =           {https://www.cs.toronto.edu/~hinton/science.pdf},
}

@Article{Atzberger2013,
  author  = {Paul J. Atzberger},
  journal = {Physica D: Nonlinear Phenomena},
  title   = {Incorporating shear into stochastic Eulerian-Lagrangian methods for rheological studies of complex fluids and soft materials},
  year    = {2013},
  issn    = {0167-2789},
  pages   = {57-70},
  volume  = {265},
  doi     = {10.1016/j.physd.2013.09.002},
  url     = {https://doi.org/10.1016/j.physd.2013.09.002},
}

@Misc{Abadi2015,
  author = {Mart\'{i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  note   = {Software available from tensorflow.org},
  title  = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  year   = {2015},
  url    = {https://www.tensorflow.org/},
}

@Book{Bird1987,
  author    = {Bird,R.B., Curtiss, C. F., Armstrong, R. C., Hassager, O.},
  publisher = {Wiley-Interscience},
  title     = {Dynamics of Polymeric Liquids : Volume II Kinetic Theory},
  year      = {1987},
  timestamp = {2009.07.29},
}

@Article{Lubensky1997,
  author    = {Lubensky, T. C.},
  journal   = {Solid State Communications},
  title     = {Soft condensed matter physics},
  year      = {1997},
  issn      = {0038-1098},
  number    = {2-3},
  pages     = {187-197--},
  volume    = {102},
  timestamp = {2009.07.14},
}

@Article{Kimura2009,
  author    = {Kimura, Y.},
  journal   = {J. Phys. Soc. Jpn.},
  title     = {Microrheology of Soft Matter},
  year      = {2009},
  issn      = {0031-9015},
  number    = {4},
  pages     = {8--},
  volume    = {78},
  address   = {Kyushu Univ, Dept Phys, Sch Sci, Higashi Ku, Fukuoka 8128581, Japan. Kimura, Y, Kyushu Univ, Dept Phys, Sch Sci, Higashi Ku, 6-10-1 Hakozaki, Fukuoka 8128581, Japan. kimura@phys.kyushu-u.ac.jp},
  timestamp = {2009.07.14},
}

@Book{Mccammon1988,
  author    = {McCammon, J.A. and Harvey, S.C.},
  publisher = {Cambridge University Press},
  title     = {Dynamics of Proteins and Nucleic Acids},
  year      = {1988},
  isbn      = {9780521356527},
  url       = {https://books.google.com/books?id=OXDS3Oq2QYIC},
}

@Article{Karplus2002,
  author  = {Karplus, Martin and McCammon, J. Andrew},
  journal = {Nature Structural Biology},
  title   = {Molecular dynamics simulations of biomolecules},
  year    = {2002},
  issn    = {1545-9985},
  month   = sep,
  number  = {9},
  pages   = {646--652},
  volume  = {9},
  url     = {https://doi.org/10.1038/nsb0902-646},
}

@Article{Karplus1983,
  author    = {Brooks, Bernard R and Bruccoleri, Robert E and Olafson, Barry D and States, David J and Swaminathan, S a and Karplus, Martin},
  journal   = {Journal of computational chemistry},
  title     = {CHARMM: a program for macromolecular energy, minimization, and dynamics calculations},
  year      = {1983},
  number    = {2},
  pages     = {187--217},
  volume    = {4},
  doi       = {10.1002/jcc.540040211},
  publisher = {Wiley Online Library},
}

@Article{Plimpton1995,
  author    = {Plimpton, Steve},
  journal   = {Journal of Computational Physics},
  title     = {Fast Parallel Algorithms for Short-Range Molecular Dynamics},
  year      = {1995},
  issn      = {0021-9991},
  month     = mar,
  number    = {1},
  pages     = {1--19},
  volume    = {117},
  doi       = {10.2172/10176421},
  timestamp = {2015.06.12},
  url       = {http://www.sciencedirect.com/science/article/pii/
                  S002199918571039X},
}

@Book{Richardson2007,
  author    = {Richardson, Lewis Fry},
  publisher = {Cambridge university press},
  title     = {Weather prediction by numerical process},
  year      = {2007},
}

@Article{Bauer2015,
  author  = {Bauer, Peter and Thorpe, Alan and Brunet, Gilbert},
  journal = {Nature},
  title   = {The quiet revolution of numerical weather prediction},
  year    = {2015},
  issn    = {1476-4687},
  month   = sep,
  number  = {7567},
  pages   = {47--55},
  volume  = {525},
  doi     = {10.1038/nature14956},
  url     = {https://doi.org/10.1038/nature14956},
}

@Article{Lusk2011,
  author  = {Lusk, Mark T. and Mattsson, Ann E.},
  journal = {MRS Bulletin},
  title   = {High-performance computing for materials design to advance energy science},
  year    = {2011},
  issn    = {1938-1425},
  month   = mar,
  number  = {3},
  pages   = {169--174},
  volume  = {36},
  doi     = {10.1557/mrs.2011.30},
  url     = {https://doi.org/10.1557/mrs.2011.30},
}

@Article{Sanbonmatsu2007,
  author    = {Sanbonmatsu, KY and Tung, C-S},
  journal   = {Journal of structural biology},
  title     = {High performance computing in biology: multimillion atom simulations of nanoscale systems},
  year      = {2007},
  number    = {3},
  pages     = {470--480},
  volume    = {157},
  doi       = {10.1016/j.jsb.2006.10.023},
  publisher = {Elsevier},
}

@Article{Washington2009,
  author  = {Washington, Warren M and Buja, Lawrence and Craig, Anthony},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  title   = {The computational future for climate and Earth system models: on the path to petaflop and beyond},
  year    = {2009},
  number  = {1890},
  pages   = {833-846},
  volume  = {367},
  doi     = {10.1098/rsta.2008.0219},
  url     = {https://royalsocietypublishing.org/doi/abs/10.1098/
                  rsta.2008.0219},
}

@Article{Pan2021,
  author  = {Pan, Jie},
  journal = {Nature Computational Science},
  title   = {Scaling up system size in materials simulation},
  year    = {2021},
  issn    = {2662-8457},
  month   = feb,
  number  = {2},
  pages   = {95--95},
  volume  = {1},
  doi     = {10.1038/s43588-021-00034-x},
  url     = {https://doi.org/10.1038/s43588-021-00034-x},
}

@InBook{Murr2016,
  author    = {Murr, Lawrence E.},
  pages     = {1--15},
  publisher = {Springer International Publishing},
  title     = {Computer Simulations in Materials Science and Engineering},
  year      = {2016},
  address   = {Cham},
  isbn      = {978-3-319-01905-5},
  booktitle = {Handbook of Materials Structures, Properties, Processing and Performance},
  doi       = {10.1007/978-3-319-01905-5_60-2},
  url       = {https://doi.org/10.1007/978-3-319-01905-5_60-2},
}

@Article{Giessen2020,
  author    = {Erik van der Giessen and Peter A Schultz and Nicolas Bertin and Vasily V Bulatov and Wei Cai and G{\'{a}}bor Cs{\'{a}}nyi and Stephen M Foiles and M G D Geers and Carlos Gonz{\'{a}}lez and Markus Hütter and Woo Kyun Kim and Dennis M Kochmann and Javier LLorca and Ann E Mattsson and Jörg Rottler and Alexander Shluger and Ryan B Sills and Ingo Steinbach and Alejandro Strachan and Ellad B Tadmor},
  journal   = {Modelling and Simulation in Materials Science and Engineering},
  title     = {Roadmap on multiscale materials modeling},
  year      = {2020},
  month     = {mar},
  number    = {4},
  pages     = {043001},
  volume    = {28},
  doi       = {10.1088/1361-651x/ab7150},
  publisher = {{IOP} Publishing},
  url       = {https://doi.org/10.1088/1361-651x/ab7150},
}

@Book{Scholkopf2001,
  author    = {Scholkopf, Bernhard and Smola, Alexander J.},
  publisher = {MIT Press},
  title     = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
  year      = {2001},
  address   = {Cambridge, MA, USA},
  isbn      = {0262194759},
  timestamp = {2020.09.11},
}

@InBook{Rasmussen2004,
  author    = {Rasmussen, Carl Edward},
  editor    = {Bousquet, Olivier and von Luxburg, Ulrike and R{\"a}tsch, Gunnar},
  pages     = {63--71},
  publisher = {Springer Berlin Heidelberg},
  title     = {Gaussian Processes in Machine Learning},
  year      = {2004},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-540-28650-9},
  booktitle = {Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2 - 14, 2003, T{\"u}bingen, Germany, August 4 - 16, 2003, Revised Lectures},
  doi       = {10.1007/978-3-540-28650-9_4},
  timestamp = {2020.09.16},
  url       = {https://doi.org/10.1007/978-3-540-28650-9_4},
}

@Book{Hastie2001,
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  publisher = {Springer New York Inc.},
  title     = {Elements of Statistical Learning},
  year      = {2001},
  address   = {New York, NY, USA},
  series    = {Springer Series in Statistics},
  timestamp = {2018.09.21},
}

@InCollection{Paszke2019,
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  publisher = {Curran Associates, Inc.},
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {8024--8035},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-
                  style-high-performance-deep-learning-library.pdf},
}

@Article{Derjaguin1941,
  author    = {Derjaguin, B.; Landau, L.},
  journal   = {Acta Physico Chemica URSS},
  title     = {Theory of the stability of strongly charged lyophobic sols and of the adhesion of strongly charged particles in solutions of electrolytes},
  year      = {1941},
  number    = {14},
  volume    = {633},
  doi       = {10.1016/0079-6816(93)90013-l},
  timestamp = {2017.04.17},
}

@Book{Doi2013,
  author    = {Doi, Masao},
  publisher = {Oxford University Press},
  title     = {Soft matter physics},
  year      = {2013},
}

@Book{Jones2002,
  author    = {Jones, R.A.L. and Jones, R.A.L. and R Jones, P.},
  publisher = {OUP Oxford},
  title     = {Soft Condensed Matter},
  year      = {2002},
  isbn      = {9780198505891},
  series    = {Oxford Master Series in Physics},
  url       = {https://books.google.com/books?id=Hl\_HBPUvoNsC},
}

@Article{Atzberger2018b,
  author    = {Sidhu, Inderbir and Frischknecht, Amalie L. and Atzberger, Paul J.},
  journal   = {ACS Omega},
  title     = {Electrostatics of Nanoparticle-Wall Interactions within Nanochannels: Role of Double-Layer Structure and Ion-Ion Correlations},
  year      = {2018},
  issn      = {2470-1343},
  month     = sep,
  number    = {9},
  pages     = {11340--11353},
  volume    = {3},
  doi       = {10.1021/acsomega.8b01393},
  publisher = {American Chemical Society},
  url       = {https://doi.org/10.1021/acsomega.8b01393},
}

@Article{Smoluchowski1906,
  author  = {Smoluchowski, VMV},
  journal = {Ann. Phys},
  title   = {Drei Vorträge über Diffusion, Brownsche Molekularbewegung und Koagulation von Kolloidteilchen},
  year    = {1906},
  pages   = {756},
  volume  = {21},
  url     = {https://jbc.bj.uj.edu.pl/Content/387533/PDF/
                  FIZART_SMOLUCHOWSKI_00093.pdf},
}

@Article{LopezAtzberger2022,
  author  = {Lopez, Ryan and Atzberger, Paul J},
  journal = {arXiv preprint arXiv:2206.05183},
  title   = {GD-VAEs: Geometric Dynamic Variational Autoencoders for Learning Nonlinear Dynamics and Dimension Reductions},
  year    = {2022},
  url     = {https://arxiv.org/abs/2206.05183},
}

@Article{zenodo,
  author       = {Paul J. Atzberger},
  journal      = {Zenodo},
  title        = {GD-VAE Package v1.0.0},
  year         = {2023},
  abstractnote = {GD-VAE Package},
  doi          = {10.5281/zenodo.7945271},
  publisher    = {Zenodo},
  url          = {http://dx.doi.org/10.5281/zenodo.7945271},
}

@InProceedings{Collobert2011,
  author    = {R. Collobert and K. Kavukcuoglu and C. Farabet},
  booktitle = {BigLearn, NIPS Workshop},
  title     = {Torch7: A Matlab-like Environment for Machine Learning},
  year      = {2011},
}

@Article{Atzberger2023,
  author  = {Stinis, Panos and Daskalakis, Constantinos and Atzberger, Paul J},
  journal = {arXiv preprint arXiv:2302.03663},
  title   = {SDYN-GANs: Adversarial Learning Methods for Multistep Generative Models for General Order Stochastic Dynamics},
  year    = {2023},
  url     = {https://arxiv.org/abs/2302.03663},
}

@Article{Nielsen2000,
  author    = {Nielsen, Jan Nygaard and Madsen, Henrik and Young, Peter C},
  journal   = {Annual Reviews in Control},
  title     = {Parameter estimation in stochastic differential equations: an overview},
  year      = {2000},
  pages     = {83--94},
  volume    = {24},
  publisher = {Elsevier},
}


@Book{Nelles2013,
  author    = {Nelles, Oliver},
  publisher = {Springer Science \& Business Media},
  title     = {Nonlinear system identification: from classical approaches to neural networks and fuzzy models},
  year      = {2013},
  doi       = {10.1007/978-3-662-04323-3},
  url       = {https://doi.org/10.1007/978-3-662-04323-3},
}

@Article{Sjoeberg1995,
  author   = {Jonas Sjöberg and Qinghua Zhang and Lennart Ljung and Albert Benveniste and Bernard Delyon and Pierre-Yves Glorennec and Håkan Hjalmarsson and Anatoli Juditsky},
  journal  = {Automatica},
  title    = {Nonlinear black-box modeling in system identification: a unified overview},
  year     = {1995},
  issn     = {0005-1098},
  note     = {Trends in System Identification},
  number   = {12},
  pages    = {1691 - 1724},
  volume   = {31},
  abstract = {A nonlinear black-box structure for a dynamical
                   system is a model structure that is prepared to
                   describe virtually any nonlinear dynamics. There has
                   been considerable recent interest in this area, with
                   structures based on neural networks, radial basis
                   networks, wavelet networks and hinging hyperplanes,
                   as well as wavelet-transform-based methods and models
                   based on fuzzy sets and fuzzy rules. This paper
                   describes all these approaches in a common framework,
                   from a user's perspective. It focuses on what are the
                   common features in the different approaches, the
                   choices that have to be made and what considerations
                   are relevant for a successful system-identification
                   application of these techniques. It is pointed out
                   that the nonlinear structures can be seen as a
                   concatenation of a mapping form observed data to a
                   regression vector and a nonlinear mapping from the
                   regressor space to the output space. These mappings
                   are discussed separately. The latter mapping is
                   usually formed as a basis function expansion. The
                   basis functions are typically formed from one simple
                   scalar function, which is modified in terms of scale
                   and location. The expansion from the scalar argument
                   to the regressor space is achieved by a radial- or a
                   ridge-type approach. Basic techniques for estimating
                   the parameters in the structures are criterion
                   minimization, as well as two-step procedures, where
                   first the relevant basis functions are determined,
                   using data, and then a linear least-squares step to
                   determine the coordinates of the function
                   approximation. A particular problem is to deal with
                   the large number of potentially necessary parameters.
                   This is handled by making the number of ‘used’
                   parameters considerably less than the number of
                   ‘offered’ parameters, by regularization,
                   shrinking, pruning or regressor selection.},
  doi      = {10.1016/0005-1098(95)00120-8},
  url      = {http://www.sciencedirect.com/science/article/pii/0005109895001208},
}

@article{Chiuso2019,
  author =        {Chiuso, A. and Pillonetto, G.},
  journal =       {Annual Review of Control, Robotics, and Autonomous
                   Systems},
  number =        {1},
  pages =         {281-304},
  title =         {System Identification: A Machine Learning
                   Perspective},
  volume =        {2},
  year =          {2019},
  abstract =      {Estimation of functions from sparse and noisy data is
                   a central theme in machine learning. In the last few
                   years, many algorithms have been developed that
                   exploit Tikhonov regularization theory and
                   reproducing kernel Hilbert spaces. These are the
                   so-called kernel-based methods, which include
                   powerful approaches like regularization networks,
                   support vector machines, and Gaussian regression.
                   Recently, these techniques have also gained
                   popularity in the system identification community. In
                   both linear and nonlinear settings, kernels that
                   incorporate information on dynamic systems, such as
                   the smoothness and stability of the input–output
                   map, can challenge consolidated approaches based on
                   parametric model structures. In the classical
                   parametric setting, the complexity of the model (the
                   model order) needs to be chosen, typically from a
                   finite family of alternatives, by trading bias and
                   variance. This (discrete) model order selection step
                   may be critical, especially when the true model does
                   not belong to the model class. In
                   regularization-based approaches, model complexity is
                   controlled by tuning (continuous) regularization
                   parameters, making the model selection step more
                   robust. In this article, we review these new
                   kernel-based system identification approaches and
                   discuss extensions based on nuclear and norms.},
  doi =           {10.1146/annurev-control-053018-023744},
  url =           {https://doi.org/10.1146/annurev-control-053018-023744},
}

@article{Hong2008,
  author =        {X. Hong and R.J. Mitchell and S. Chen and C.J. Harris and
                   K. Li and G.W. Irwin},
  journal =       {International Journal of Systems Science},
  number =        {10},
  pages =         {925-946},
  publisher =     {Taylor \& Francis},
  title =         {Model selection approaches for non-linear system
                   identification: a review},
  volume =        {39},
  year =          {2008},
  abstract =      {The identification of non-linear systems using only
                   observed finite datasets has become a mature research
                   area over the last two decades. A class of
                   linear-in-the-parameter models with universal
                   approximation capabilities have been intensively
                   studied and widely used due to the availability of
                   many linear-learning algorithms and their inherent
                   convergence conditions. This article presents a
                   systematic overview of basic research on model
                   selection approaches for linear-in-the-parameter
                   models. One of the fundamental problems in non-linear
                   system identification is to find the minimal model
                   with the best model generalisation performance from
                   observational data only. The important concepts in
                   achieving good model generalisation used in various
                   non-linear system-identification algorithms are first
                   reviewed, including Bayesian parameter regularisation
                   and models selective criteria based on the cross
                   validation and experimental design. A significant
                   advance in machine learning has been the development
                   of the support vector machine as a means for
                   identifying kernel models based on the structural
                   risk minimisation principle. The developments on the
                   convex optimisation-based model construction
                   algorithms including the support vector regression
                   algorithms are outlined. Input selection algorithms
                   and on-line system identification algorithms are also
                   included in this review. Finally, some industrial
                   applications of non-linear models are discussed.},
  doi =           {10.1080/00207720802083018},
  url =           {https://doi.org/10.1080/00207720802083018},
}

@inbook{Kutz_Brunton_book_ch_ROMs_2019,
  author =        {Brunton, Steven L. and Kutz, J. Nathan},
  booktitle =     {Data-Driven Science and Engineering: Machine
                   Learning, Dynamical Systems, and Control},
  pages =         {375–402},
  publisher =     {Cambridge University Press},
  title =         {Reduced Order Models (ROMs)},
  year =          {2019},
  doi =           {10.1017/9781108380690.012},
  timestamp =     {2020.09.25},
}

@article{Mallet_Coifman_Manifold_Learning_LVM_Dyn_Sys_2015,
  author =        {R. {Talmon} and S. {Mallat} and H. {Zaveri} and
                   R. R. {Coifman}},
  journal =       {IEEE Transactions on Signal Processing},
  number =        {15},
  pages =         {3843-3856},
  title =         {Manifold Learning for Latent Variable Inference in
                   Dynamical Systems},
  volume =        {63},
  year =          {2015},
  doi =           {10.1109/TSP.2015.2432731},
}

@article{Kutz_Lusch_Nonlinear_Embeddings_2018,
  author =        {Lusch, Bethany and Kutz, J. Nathan and
                   Brunton, Steven L.},
  journal =       {Nature Communications},
  month =         nov,
  number =        {1},
  pages =         {4950},
  title =         {Deep learning for universal linear embeddings of
                   nonlinear dynamics},
  volume =        {9},
  year =          {2018},
  abstract =      {Identifying coordinate transformations that make
                   strongly nonlinear dynamics approximately linear has
                   the potential to enable nonlinear prediction,
                   estimation, and control using linear theory. The
                   Koopman operator is a leading data-driven embedding,
                   and its eigenfunctions provide intrinsic coordinates
                   that globally linearize the dynamics. However,
                   identifying and representing these eigenfunctions has
                   proven challenging. This work leverages deep learning
                   to discover representations of Koopman eigenfunctions
                   from data. Our network is parsimonious and
                   interpretable by construction, embedding the dynamics
                   on a low-dimensional manifold. We identify nonlinear
                   coordinates on which the dynamics are globally linear
                   using a modified auto-encoder. We also generalize
                   Koopman representations to include a ubiquitous class
                   of systems with continuous spectra. Our framework
                   parametrizes the continuous frequency using an
                   auxiliary network, enabling a compact and efficient
                   embedding, while connecting our models to decades of
                   asymptotics. Thus, we benefit from the power of deep
                   learning, while retaining the physical
                   interpretability of Koopman embeddings.},
  issn =          {2041-1723},
  timestamp =     {2020.08.31},
  url =           {https://doi.org/10.1038/s41467-018-07210-0},
}

@article{Mezic2013,
  author =        {Mezić, Igor},
  journal =       {Annual Review of Fluid Mechanics},
  number =        {1},
  pages =         {357-378},
  title =         {Analysis of Fluid Flows via Spectral Properties of
                   the Koopman Operator},
  volume =        {45},
  year =          {2013},
  abstract =      {This article reviews theory and applications of
                   Koopman modes in fluid mechanics. Koopman mode
                   decomposition is based on the surprising fact,
                   discovered in Mezić (2005), that normal modes of
                   linear oscillations have their natural
                   analogs—Koopman modes—in the context of nonlinear
                   dynamics. To pursue this analogy, one must change the
                   representation of the system from the state-space
                   representation to the dynamics governed by the linear
                   Koopman operator on an infinite-dimensional space of
                   observables. Whereas Koopman in his original paper
                   dealt only with measure-preserving transformations,
                   the discussion here is predominantly on dissipative
                   systems arising from Navier-Stokes evolution. The
                   analysis is based on spectral properties of the
                   Koopman operator. Aspects of point and continuous
                   parts of the spectrum are discussed. The point
                   spectrum corresponds to isolated frequencies of
                   oscillation present in the fluid flow, and also to
                   growth rates of stable and unstable modes. The
                   continuous part of the spectrum corresponds to
                   chaotic motion on the attractor. A method of
                   computation of the spectrum and the associated
                   Koopman modes is discussed in terms of generalized
                   Laplace analysis. When applied to a generic
                   observable, this method uncovers the full point
                   spectrum. A computational alternative is given by
                   Arnoldi-type methods, leading to so-called dynamic
                   mode decomposition, and I discuss the connection and
                   differences between these two methods. A number of
                   applications are reviewed in which decompositions of
                   this type have been pursued. Koopman mode theory
                   unifies and provides a rigorous background for a
                   number of different concepts that have been advanced
                   in fluid mechanics, including global mode analysis,
                   triple decomposition, and dynamic mode
                   decomposition.},
  doi =           {10.1146/annurev-fluid-011212-140652},
  timestamp =     {2020.09.30},
  url =           {https://doi.org/10.1146/annurev-fluid-011212-140652},
}

@article{Ohlberger_Redcuced_Basis_Review_2016,
  author =        {Mario Ohlberger and Stephan Rave},
  journal =       {Proceedings of the Conference Algoritmy},
  pages =         {1--12},
  title =         {Reduced Basis Methods: Success, Limitations and
                   Future Challenges},
  year =          {2016},
  abstract =      {Parametric model order reduction using reduced basis
                   methods can be an effective tool for obtaining
                   quickly solvable reduced order model  of
                   parametrized partial differential equation problems.
                   With speedups that can reach several orders of
                   magnitude, reduced basis methods enable high fidelity
                   real-time simulations of complex systems and
                   dramatically reduce the computational costs in
                   many-query applications. In this contribution we
                   analyze the methodology, mainly focussing on the
                   theoretical aspects of the approach. In particular we
                   discuss what is known about the convergence
                   properties of these methods: when they succeed and
                   when they are bound to fail. Moreover, we highlight
                   some recent approaches employing nonlinear 
                   approximation techniques which aim to overcome the
                   current limitations of reduced basis methods.},
  timestamp =     {2020.09.23},
  url =           {http://www.iam.fmph.uniba.sk/amuc/ojs/index.php/algoritmy/
                  article/view/389},
}

@article{Hesthaven2016,
  author =        {Jan S. Hesthaven and Gianluigi Rozza and
                   Benjamin Stamm},
  pages =         {27-43},
  title =         {Reduced Basis Methods},
  year =          {2016},
  doi =           {10.1007/978-3-319-22470-1_3},
  issn =          {2191-8198},
  timestamp =     {2020.09.16},
}

@article{Crutchfield_Dynamics_Symbolic_1987,
  author =        {J. Crutchfield and Bruce S. McNamara},
  journal =       {Complex Syst.},
  title =         {Equations of Motion from a Data Series},
  volume =        {1},
  year =          {1987},
}

@inbook{DeVore_Reduced_Basis_Methods_Ch_2017,
  author =        {Ronald A. DeVore},
  booktitle =     {Model Reduction and Approximation},
  chapter =       {Chapter 3: The Theoretical Foundation of Reduced
                   Basis Methods},
  pages =         {137-168},
  publisher =     {SIAM},
  title =         {Model Reduction and Approximation: Theory and
                   Algorithms},
  year =          {2017},
  doi =           {10.1137/1.9781611974829.ch3},
  timestamp =     {2020.09.17},
  url =           {https://epubs.siam.org/doi/abs/10.1137/1.9781611974829.ch3},
}

@article{Kutz_DNN_Time_Step_Constraints_2018,
  author =        {Samuel H. Rudy , J. Nathan Kutz, Steven L. Brunton},
  journal =       {arXiv:1808:02578},
  title =         {Deep learning of dynamics and signal-noise
                   decomposition with time-stepping constraints},
  year =          {2018},
  url =           {https://doi.org/10.1016/j.jcp.2019.06.056},
}

@article{Schmid2010,
  author =        {Schmid, Peter J.},
  journal =       {Journal of Fluid Mechanics},
  pages =         {5–28},
  publisher =     {Cambridge University Press},
  title =         {Dynamic mode decomposition of numerical and
                   experimental data},
  volume =        {656},
  year =          {2010},
  doi =           {10.1017/S0022112010001217},
  timestamp =     {2020.09.30},
  url =           {https://doi.org/10.1017/S0022112010001217},
}

@article{Archer2015,
  author =        {Archer, Evan and Park, Il Memming and Buesing, Lars and
                   Cunningham, John and Paninski, Liam},
  journal =       {arXiv preprint arXiv:1511.07367},
  title =         {Black box variational inference for state space
                   models},
  year =          {2015},
  url =           {https://arxiv.org/abs/1511.07367},
}

@article{Jordan2023,
  author =        {Mania, Horia and Jordan, Michael I and
                   Recht, Benjamin},
  journal =       {arXiv preprint arXiv:2006.10277},
  title =         {Active learning for nonlinear system identification
                   with guarantees},
  year =          {2020},
  timestamp =     {2020.09.14},
  url =           {https://arxiv.org/pdf/2006.10277.pdf},
}

@InProceedings{Ghahramani1998,
  author    = {Zoubin Ghahramani and Sam T. Roweis},
  booktitle = {Advances in Neural Information Processing Systems 11, {[NIPS} Conference, Denver, Colorado, USA, November 30 - December 5, 1998]},
  title     = {Learning Nonlinear Dynamical Systems Using an {EM} Algorithm},
  year      = {1998},
  editor    = {Michael J. Kearns and Sara A. Solla and David A. Cohn},
  pages     = {431--437},
  publisher = {The {MIT} Press},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {2020.09.14},
  url       = {https://proceedings.neurips.cc/paper/1998/hash/0ebcc77dc72360d0eb8e9504c78d38bd-Abstract.html},
}

@article{Carlberg2020,
  author =        {Kookjin Lee and Kevin T. Carlberg},
  journal =       {Journal of Computational Physics},
  pages =         {108973},
  title =         {Model reduction of dynamical systems on nonlinear
                   manifolds using deep convolutional autoencoders},
  volume =        {404},
  year =          {2020},
  abstract =      {Nearly all model-reduction techniques project the
                   governing equations onto a linear subspace of the
                   original state space. Such subspaces are typically
                   computed using methods such as balanced truncation,
                   rational interpolation, the reduced-basis method, and
                   (balanced) proper orthogonal decomposition (POD).
                   Unfortunately, restricting the state to evolve in a
                   linear subspace imposes a fundamental limitation to
                   the accuracy of the resulting reduced-order model
                   (ROM). In particular, linear-subspace ROMs can be
                   expected to produce low-dimensional models with high
                   accuracy only if the problem admits a fast decaying
                   Kolmogorov n-width (e.g., diffusion-dominated
                   problems). Unfortunately, many problems of interest
                   exhibit a slowly decaying Kolmogorov n-width (e.g.,
                   advection-dominated problems). To address this, we
                   propose a novel framework for projecting dynamical
                   systems onto nonlinear manifolds using
                   minimum-residual formulations at the time-continuous
                   and time-discrete levels; the former leads to
                   manifold Galerkin projection, while the latter leads
                   to manifold least-squares Petrov–Galerkin (LSPG)
                   projection. We perform analyses that provide insight
                   into the relationship between these proposed
                   approaches and classical linear-subspace
                   reduced-order models; we also derive a posteriori
                   discrete-time error bounds for the proposed
                   approaches. In addition, we propose a computationally
                   practical approach for computing the nonlinear
                   manifold, which is based on convolutional
                   autoencoders from deep learning. Finally, we
                   demonstrate the ability of the method to
                   significantly outperform even the optimal
                   linear-subspace ROM on benchmark advection-dominated
                   problems, thereby demonstrating the method's ability
                   to overcome the intrinsic n-width limitations of
                   linear subspaces.},
  doi =           {https://doi.org/10.1016/j.jcp.2019.108973},
  issn =          {0021-9991},
  timestamp =     {2020.09.22},
  url =           {http://www.sciencedirect.com/science/article/pii/
                  S0021999119306783},
}

@article{Karniadakis2018,
  author =        {Maziar Raissi and George Em Karniadakis},
  journal =       {Journal of Computational Physics},
  pages =         {125 - 141},
  title =         {Hidden physics models: Machine learning of nonlinear
                   partial differential equations},
  volume =        {357},
  year =          {2018},
  abstract =      {While there is currently a lot of enthusiasm about
                   âbig dataâ, useful data is usually
                   âsmallâ and expensive to acquire. In this
                   paper, we present a new paradigm of learning partial
                   differential equations from small data. In
                   particular, we introduce hidden physics models, which
                   are essentially data-efficient learning machines
                   capable of leveraging the underlying laws of physics,
                   expressed by time dependent and nonlinear partial
                   differential equations, to extract patterns from
                   high-dimensional data generated from experiments. The
                   proposed methodology may be applied to the problem of
                   learning, system identification, or data-driven
                   discovery of partial differential equations. Our
                   framework relies on Gaussian processes, a powerful
                   tool for probabilistic inference over functions, that
                   enables us to strike a balance between model
                   complexity and data fitting. The effectiveness of the
                   proposed approach is demonstrated through a variety
                   of canonical problems, spanning a number of
                   scientific domains, including the NavierâStokes,
                   SchrÃ¶dinger, KuramotoâSivashinsky, and time
                   dependent linear fractional equations. The
                   methodology provides a promising new direction for
                   harnessing the long-standing developments of
                   classical methods in applied mathematics and
                   mathematical physics to design learning machines with
                   the ability to operate in complex domains without
                   requiring large quantities of data.},
  issn =          {0021-9991},
  timestamp =     {2020.08.31},
  url =           {https://arxiv.org/abs/1708.00588},
}

@article{Bertozzi2019,
  author =        {Azencot, Omri and Yin, Wotao and Bertozzi, Andrea},
  journal =       {SIAM Journal on Applied Dynamical Systems},
  number =        {3},
  pages =         {1565--1585},
  publisher =     {SIAM},
  title =         {Consistent dynamic mode decomposition},
  volume =        {18},
  year =          {2019},
  url =           {https://www.math.ucla.edu/~bertozzi/papers/CDMD_SIADS.pdf},
}

@article{Perdikaris2018,
  author =        {Yibo Yang and Paris Perdikaris},
  journal =       {arXiv preprint arXiv:1812.03511},
  title =         {Physics-informed deep generative models},
  year =          {2018},
}

@article{Kalman1960,
  author =        {Kalman, R. E.},
  journal =       {Journal of Basic Engineering},
  month =         {03},
  number =        {1},
  pages =         {35-45},
  title =         {{A New Approach to Linear Filtering and Prediction
                   Problems}},
  volume =        {82},
  year =          {1960},
  abstract =      {{The classical filtering and prediction problem is
                   re-examined using the Bode-Shannon representation of
                   random processes and the “state-transition”
                   method of analysis of dynamic systems. New results
                   are: (1) The formulation and methods of solution of
                   the problem apply without modification to stationary
                   and nonstationary statistics and to growing-memory
                   and infinite-memory filters. (2) A nonlinear
                   difference (or differential) equation is derived for
                   the covariance matrix of the optimal estimation
                   error. From the solution of this equation the
                   co-efficients of the difference (or differential)
                   equation of the optimal linear filter are obtained
                   without further calculations. (3) The filtering
                   problem is shown to be the dual of the noise-free
                   regulator problem. The new method developed here is
                   applied to two well-known problems, confirming and
                   extending earlier results. The discussion is largely
                   self-contained and proceeds from first principles;
                   basic concepts of the theory of random processes are
                   reviewed in the Appendix.}},
  doi =           {10.1115/1.3662552},
  issn =          {0021-9223},
  url =           {https://doi.org/10.1115/1.3662552},
}

@article{DelMoral1997,
  author =        {Pierre {Del Moral}},
  journal =       {Comptes Rendus de l'Académie des Sciences - Series I
                   - Mathematics},
  number =        {6},
  pages =         {653 - 658},
  title =         {Nonlinear filtering: Interacting particle resolution},
  volume =        {325},
  year =          {1997},
  abstract =      {In this Note, we study interacting particle
                   approximations of discrete time and measure valued
                   dynamical systems. Such systems have arisen in such
                   diverse scientific disciplines as in Propagation of
                   Chaos Theory (see [12] and [19]), and in Nonlinear
                   Filtering Theory. The main contribution of this Note
                   is to prove the convergences to the optimal filter of
                   such approximations, yielding what seemed to be the
                   first mathematically well-founded convergence results
                   for such approximations of the nonlinear filtering
                   equations. This new treatment was influenced
                   primarily by the development of genetic algorithms
                   (see [16] and [3]), and secondarily by the papers of
                   H. Kunita and L. Stettner, [17] and [18]
                   respectively. Résumé Cette Note présente une
                   méthode de résolution particulaire de systèmes
                   dynamiques à valeurs mesures, basée sur la
                   simulation de systèmes de particules en interaction.
                   Ces techniques permettent d'aborder l'étude de
                   certaines équations étudiées dans la Théorie de
                   la propagation du chaos (voir [12] et [19]), et tout
                   particulièrement les équations du filtrage
                   non-linéaire. Un des principaux résultats de cette
                   Note concerne la convergence vers le filtre optimal
                   de ces approximations particulaires. Ces résultats
                   constituent, à la connaissance de l'auteur, la
                   première démonstration de convergence de telles
                   approximations des équations du filtrage
                   non-linéaire. Cette nouvelle approche a été
                   influencée par le développement des Algorithmes
                   Génétiques (voir [16] et [3]), et par les articles
                   de H. Kunita et L. Stettner,[17] et [18]
                   respectivement.},
  doi =           {https://doi.org/10.1016/S0764-4442(97)84778-7},
  issn =          {0764-4442},
  url =           {http://www.sciencedirect.com/science/article/pii/
                  S0764444297847787},
}

@InProceedings{Godsill2019,
  author    = {S. {Godsill}},
  booktitle = {Proc. Speech and Signal Processing (ICASSP) ICASSP 2019 - 2019 IEEE Int. Conf. Acoustics},
  title     = {Particle Filtering: the First 25 Years and beyond},
  year      = {2019},
  pages     = {7760--7764},
  doi       = {10.1109/ICASSP.2019.8683411},
  timestamp = {2020.09.14},
  url       = {https://doi.org/10.1109/ICASSP.2019.8683411},
}

@InProceedings{VanDerMerwe2000,
  author    = {Van Der Merwe, Rudolph and Doucet, Arnaud and De Freitas, Nando and Wan, Eric},
  booktitle = {Proceedings of the 13th International Conference on Neural Information Processing Systems},
  title     = {The Unscented Particle Filter},
  year      = {2000},
  address   = {Cambridge, MA, USA},
  pages     = {563–569},
  publisher = {MIT Press},
  series    = {NIPS'00},
  abstract  = {In this paper, we propose a new particle filter based
                   on sequential importance sampling. The algorithm uses
                   a bank of unscented filters to obtain the importance
                   proposal distribution. This proposal has two very
                   "nice" properties. Firstly, it makes efficient use of
                   the latest available information and, secondly, it
                   can have heavy tails. As a result, we find that the
                   algorithm outperforms standard particle filtering and
                   other nonlinear filtering methods very substantially.
                   This experimental finding is in agreement with the
                   theoretical convergence proof for the algorithm. The
                   algorithm also includes resampling and (possibly)
                   Markov chain Monte Carlo (MCMC) steps.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f5c3dd7514bf620a1b85450d2ae374b1-Paper.pdf},
}

@inproceedings{Wan2000,
  author =        {E. A. {Wan} and R. {Van Der Merwe}},
  booktitle =     {Proceedings of the IEEE 2000 Adaptive Systems for
                   Signal Processing, Communications, and Control
                   Symposium (Cat. No.00EX373)},
  pages =         {153-158},
  title =         {The unscented Kalman filter for nonlinear estimation},
  year =          {2000},
  doi =           {10.1109/ASSPCC.2000.882463},
}

@article{Chatterjee2000,
  author =        {Chatterjee, Anindya},
  journal =       {Current Science},
  month =         sep,
  number =        {7},
  pages =         {808--817},
  publisher =     {Temporary Publisher},
  title =         {An introduction to the proper orthogonal
                   decomposition},
  volume =        {78},
  year =          {2000},
  abstract =      {[A tutorial is presented on the Proper Orthogonal
                   Decomposition (POD), which finds applications in
                   computationally processing large amounts of
                   high-dimensional data with the aim of obtaining
                   low-dimensional descriptions that capture much of the
                   phenomena of interest. The discrete version of the
                   POD, which is the singular value decomposition (SVD)
                   of matrices, is described in some detail. The
                   continuous version of the POD is outlined. Low-rank
                   approximations to data using the SVD are discussed.
                   The SVD and the eigenvalue decomposition are
                   compared. Two geometric interpretations of the
                   SVD/POD are given. Computational strategies (using
                   standard software) are mentioned. Two numerical
                   examples are provided: one shows low-rank
                   approximations of a surface, and the other
                   demonstrates simple a posteriori analysis of data
                   from a simulated vibroimpact system. Some relevant
                   computer code is supplied.]},
  issn =          {00113891},
  timestamp =     {2020.09.10},
  url =           {http://www.jstor.org/stable/24103957},
}

@article{Mendez2018,
  author =        {Miguel Alfonso Mendez and Mikhael Balabane and
                   Jean Marie Buchlin},
  title =         {Multi-scale proper orthogonal decomposition (mPOD)},
  year =          {2018},
  doi =           {10.1063/1.5043720},
  timestamp =     {2020.09.11},
}

@book{Kutz2016,
  address =       {Philadelphia, PA},
  author =        {Kutz, J. Nathan and Brunton, Steven L. and
                   Brunton, Bingni W. and Proctor, Joshua L.},
  publisher =     {Society for Industrial and Applied Mathematics},
  title =         {Dynamic Mode Decomposition},
  year =          {2016},
  doi =           {10.1137/1.9781611974508},
  url =           {https://epubs.siam.org/doi/abs/10.1137/1.9781611974508},
}

@Article{Kutz2014,
  author    = {Jonathan H. Tu and Clarence W. Rowley and Dirk M. Luchtenburg and Steven L. Brunton and J. Nathan Kutz},
  journal   = {Journal of Computational Dynamics},
  title     = {On dynamic mode decomposition: Theory and applications},
  year      = {2014},
  doi       = {10.3934/jcd.2014.1.391},
  timestamp = {2020.09.10},
  url       = {https://doi.org/10.3934/jcd.2014.1.391},
}

@article{Das2019,
  author =        {Suddhasattwa Das and Dimitrios Giannakis},
  pages =         {1107-1145},
  title =         {Delay-Coordinate Maps and the Spectra of Koopman
                   Operators},
  volume =        {175},
  year =          {2019},
  doi =           {10.1007/s10955-019-02272-w},
  issn =          {0022-4715},
  timestamp =     {2020.09.11},
}

@Article{Mezic2020,
  author   = {Milan Korda and Mihai Putinar and Igor Mezic},
  journal  = {Applied and Computational Harmonic Analysis},
  title    = {Data-driven spectral analysis of the Koopman operator},
  year     = {2020},
  issn     = {1063-5203},
  number   = {2},
  pages    = {599 - 629},
  volume   = {48},
  abstract = {Starting from measured data, we develop a method to
                   compute the fine structure of the spectrum of the
                   Koopman operator with rigorous convergence
                   guarantees. The method is based on the observation
                   that, in the measure-preserving ergodic setting, the
                   moments of the spectral measure associated to a given
                   observable are computable from a single trajectory of
                   this observable. Having finitely many moments
                   available, we use the classical Christoffel–Darboux
                   kernel to separate the atomic and absolutely
                   continuous parts of the spectrum, supported by
                   convergence guarantees as the number of moments tends
                   to infinity. In addition, we propose a technique to
                   detect the singular continuous part of the spectrum
                   as well as two methods to approximate the spectral
                   measure with guaranteed convergence in the weak
                   topology, irrespective of whether the singular
                   continuous part is present or not. The proposed
                   method is simple to implement and readily applicable
                   to large-scale systems since the computational
                   complexity is dominated by inverting an N×N
                   Hermitian positive-definite Toeplitz matrix, where N
                   is the number of moments, for which efficient and
                   numerically stable algorithms exist; in particular,
                   the complexity of the approach is independent of the
                   dimension of the underlying state-space. We also show
                   how to compute, from measured data, the spectral
                   projection on a given segment of the unit circle,
                   allowing us to obtain a finite approximation of the
                   operator that explicitly takes into account the point
                   and continuous parts of the spectrum. Finally, we
                   describe a relationship between the proposed method
                   and the so-called Hankel Dynamic Mode Decomposition,
                   providing new insights into the behavior of the
                   eigenvalues of the Hankel DMD operator. A number of
                   numerical examples illustrate the approach, including
                   a study of the spectrum of the lid-driven
                   two-dimensional cavity flow.},
  doi      = {10.1016/j.acha.2018.08.002},
  url      = {https://doi.org/10.1016/j.acha.2018.08.002},
}

@article{Ljung2019,
  author =        {J. {Schoukens} and L. {Ljung}},
  journal =       {IEEE Control Systems Magazine},
  number =        {6},
  pages =         {28-99},
  title =         {Nonlinear System Identification: A User-Oriented Road
                   Map},
  volume =        {39},
  year =          {2019},
  doi =           {10.1109/MCS.2019.2938121},
}

@article{Schoen2011,
  author =        {Thomas B. Schön and Adrian Wills and Brett Ninness},
  journal =       {Automatica},
  number =        {1},
  pages =         {39 - 49},
  title =         {System identification of nonlinear state-space
                   models},
  volume =        {47},
  year =          {2011},
  abstract =      {This paper is concerned with the parameter estimation
                   of a general class of nonlinear dynamic systems in
                   state-space form. More specifically, a Maximum
                   Likelihood (ML) framework is employed and an
                   Expectation Maximisation (EM) algorithm is derived to
                   compute these ML estimates. The Expectation (E) step
                   involves solving a nonlinear state estimation
                   problem, where the smoothed estimates of the states
                   are required. This problem lends itself perfectly to
                   the particle smoother, which provides arbitrarily
                   good estimates. The maximisation (M) step is solved
                   using standard techniques from numerical optimisation
                   theory. Simulation examples demonstrate the efficacy
                   of our proposed solution.},
  doi =           {https://doi.org/10.1016/j.automatica.2010.10.013},
  issn =          {0005-1098},
  url =           {http://www.sciencedirect.com/science/article/pii/
                  S0005109810004279},
}

@article{Pawar2020,
  author =        {Suraj Pawar and Shady E. Ahmed and Omer San and
                   Adil Rasheed},
  pages =         {036602},
  title =         {Data-driven recovery of hidden physics in reduced
                   order modeling of fluid flows},
  volume =        {32},
  year =          {2020},
  doi =           {10.1063/5.0002051},
  issn =          {1070-6631},
  timestamp =     {2020.09.11},
}

@article{Saul2020,
  author =        {Saul, Lawrence K.},
  journal =       {Proceedings of the National Academy of Sciences},
  number =        {27},
  pages =         {15403--15408},
  publisher =     {National Academy of Sciences},
  title =         {A tractable latent variable model for nonlinear
                   dimensionality reduction},
  volume =        {117},
  year =          {2020},
  abstract =      {Latent variable models (LVMs) are powerful tools for
                   discovering hidden structure in data. Canonical LVMs
                   include factor analysis, which explains the
                   correlation of a large number of observed variables
                   in terms of a smaller number of unobserved ones, and
                   Gaussian mixture models, which reveal clusters of
                   data arising from an underlying multimodal
                   distribution. In this paper, we describe a
                   conceptually simple and equally effective LVM for
                   nonlinear dimensionality reduction (NLDR), where the
                   goal is to discover faithful, neighborhood-preserving
                   embeddings of high-dimensional data. Tools for NLDR
                   can help researchers across all areas of science and
                   engineering to better understand and visualize their
                   data. Our approach elevates NLDR into the family of
                   problems that can be studied by especially tractable
                   LVMs.We propose a latent variable model to discover
                   faithful low-dimensional representations of
                   high-dimensional data. The model computes a
                   low-dimensional embedding that aims to preserve
                   neighborhood relationships encoded by a sparse graph.
                   The model both leverages and extends current leading
                   approaches to this problem. Like t-distributed
                   Stochastic Neighborhood Embedding, the model can
                   produce two- and three-dimensional embeddings for
                   visualization, but it can also learn
                   higher-dimensional embeddings for other uses. Like
                   LargeVis and Uniform Manifold Approximation and
                   Projection, the model produces embeddings by
                   balancing two goals{\textemdash}pulling nearby
                   examples closer together and pushing distant examples
                   further apart. Unlike these approaches, however, the
                   latent variables in our model provide additional
                   structure that can be exploited for learning. We
                   derive an Expectation{\textendash}Maximization
                   procedure with closed-form updates that monotonically
                   improve the model{\textquoteright}s likelihood: In
                   this procedure, embeddings are iteratively adapted by
                   solving sparse, diagonally dominant systems of linear
                   equations that arise from a discrete graph Laplacian.
                   For large problems, we also develop an approximate
                   coarse-graining procedure that avoids the need for
                   negative sampling of nonadjacent nodes in the graph.
                   We demonstrate the model{\textquoteright}s
                   effectiveness on datasets of images and text.},
  doi =           {10.1073/pnas.1916012117},
  issn =          {0027-8424},
  url =           {https://www.pnas.org/content/117/27/15403},
}

@article{Baum1966,
  author =        {Baum, Leonard E. and Petrie, Ted},
  journal =       {Ann. Math. Statist.},
  month =         {12},
  number =        {6},
  pages =         {1554--1563},
  publisher =     {The Institute of Mathematical Statistics},
  title =         {Statistical Inference for Probabilistic Functions of
                   Finite State Markov Chains},
  volume =        {37},
  year =          {1966},
  doi =           {10.1214/aoms/1177699147},
  url =           {https://doi.org/10.1214/aoms/1177699147},
}

@inproceedings{Krishnan2017,
  author =        {Rahul G. Krishnan and Uri Shalit and David A. Sontag},
  booktitle =     {Proceedings of the Thirty-First {AAAI} Conference on
                   Artificial Intelligence, February 4-9, 2017, San
                   Francisco, California, {USA}},
  editor =        {Satinder P. Singh and Shaul Markovitch},
  pages =         {2101--2109},
  publisher =     {{AAAI} Press},
  title =         {Structured Inference Networks for Nonlinear State
                   Space Models},
  year =          {2017},
  abstract =      {Gaussian state space models have been used for
                   decades as generative models of sequential data. They
                   admit an intuitive probabilistic interpretation, have
                   a simple functional form, and enjoy widespread
                   adoption. We introduce a unified algorithm to
                   efficiently learn a broad class of linear and
                   non-linear state space models, including variants
                   where the emission and transition distributions are
                   modeled by deep neural networks. Our learning
                   algorithm simultaneously learns a compiled inference
                   network and the generative model, leveraging a
                   structured variational approximation parameterized by
                   recurrent neural networks to mimic the posterior
                   distribution. We apply the learning algorithm to both
                   synthetic and real-world datasets, demonstrating its
                   scalability and versatility. We find that using the
                   structured approximation to the posterior results in
                   models with significantly higher held-out
                   likelihood.},
  biburl =        {https://dblp.org/rec/conf/aaai/KrishnanSS17.bib},
  bibsource =     {dblp computer science bibliography, https://dblp.org},
  timestamp =     {2020.09.23},
  url =           {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14215},
}

@article{Kullback1951,
  author =        {Kullback, S. and Leibler, R. A.},
  journal =       {Ann. Math. Statist.},
  month =         {03},
  number =        {1},
  pages =         {79--86},
  publisher =     {The Institute of Mathematical Statistics},
  title =         {On Information and Sufficiency},
  volume =        {22},
  year =          {1951},
  doi =           {10.1214/aoms/1177729694},
  url =           {https://doi.org/10.1214/aoms/1177729694},
}

@Book{Cover2006,
  author    = {Cover, Thomas M. and Thomas, Joy A.},
  publisher = {Wiley-Interscience},
  title     = {Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)},
  year      = {2006},
  address   = {USA},
  isbn      = {0471241954},
  doi       = {10.1002/047174882X},
  url       = {http://dx.doi.org/10.1002/047174882X},
}

@article{Carlsson2008,
  author =        {Carlsson, Gunnar and Ishkhanov, Tigran and
                   de Silva, Vin and Zomorodian, Afra},
  journal =       {International Journal of Computer Vision},
  month =         jan,
  number =        {1},
  pages =         {1--12},
  title =         {On the Local Behavior of Spaces of Natural Images},
  volume =        {76},
  year =          {2008},
  abstract =      {In this study we concentrate on qualitative
                   topological analysis of the local behavior of the
                   space of natural images. To this end, we use a space
                   of 3 by 3 high-contrast patches ℳ. We develop a
                   theoretical model for the high-density 2-dimensional
                   submanifold of ℳ showing that it has the topology
                   of the Klein bottle. Using our topological software
                   package PLEX we experimentally verify our theoretical
                   conclusions. We use polynomial representation to give
                   coordinatization to various subspaces of ℳ. We find
                   the best-fitting embedding of the Klein bottle into
                   the ambient space of ℳ. Our results are currently
                   being used in developing a compression algorithm
                   based on a Klein bottle dictionary.},
  issn =          {1573-1405},
  url =           {https://doi.org/10.1007/s11263-007-0056-x},
}

@article{Whitney1944,
  author =        {Hassler Whitney},
  journal =       {Annals of Mathematics},
  number =        {2},
  pages =         {220--246},
  publisher =     {Annals of Mathematics},
  title =         {The Self-Intersections of a Smooth n-Manifold in
                   2n-Space},
  volume =        {45},
  year =          {1944},
  issn =          {0003486X},
  url =           {http://www.jstor.org/stable/1969265},
}

@Article{Atzberger2019,
  author    = {B.J. Gross and N. Trask and P. Kuberry and P.J. Atzberger},
  journal   = {Journal of Computational Physics},
  title     = {Meshfree methods on manifolds for hydrodynamic flows on curved surfaces: A Generalized Moving Least-Squares (GMLS) approach},
  year      = {2020},
  issn      = {0021-9991},
  pages     = {109340},
  volume    = {409},
  abstract  = {We utilize generalized moving least squares (GMLS) to
                   develop meshfree techniques for discretizing
                   hydrodynamic flow problems on manifolds. We use
                   exterior calculus to formulate incompressible
                   hydrodynamic equations in the Stokesian regime and
                   handle the divergence-free constraints via a
                   generalized vector potential. This provides less
                   coordinate-centric descriptions and enables the
                   development of efficient numerical methods and
                   splitting schemes for the fourth-order governing
                   equations in terms of a system of second-order
                   elliptic operators. Using a Hodge decomposition, we
                   develop methods for manifolds having spherical
                   topology. We show the methods exhibit high-order
                   convergence rates for solving hydrodynamic flows on
                   curved surfaces. The methods also provide general
                   high-order approximations for the metric, curvature,
                   and other geometric quantities of the manifold and
                   associated exterior calculus operators. The
                   approaches also can be utilized to develop high-order
                   solvers for other scalar-valued and vector-valued
                   problems on manifolds.},
  doi       = {10.1016/j.jcp.2020.109340},
  timestamp = {2020.09.10},
  url       = {https://doi.org/10.1016/j.jcp.2020.109340},
}

@article{Schmidhuber1997,
  address =       {Cambridge, MA, USA},
  author =        {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
  journal =       {Neural Comput.},
  month =         nov,
  number =        {8},
  pages =         {1735–1780},
  publisher =     {MIT Press},
  title =         {Long Short-Term Memory},
  volume =        {9},
  year =          {1997},
  abstract =      {Learning to store information over extended time
                   intervals by recurrent backpropagation takes a very
                   long time, mostly because of insufficient, decaying
                   error backflow. We briefly review Hochreiter's (1991)
                   analysis of this problem, then address it by
                   introducing a novel, efficient, gradient based method
                   called long short-term memory (LSTM). Truncating the
                   gradient where this does not do harm, LSTM can learn
                   to bridge minimal time lags in excess of 1000
                   discrete-time steps by enforcing constant error flow
                   through constant error carousels within special
                   units. Multiplicative gate units learn to open and
                   close access to the constant error flow. LSTM is
                   local in space and time; its computational complexity
                   per time step and weight is O. 1. Our experiments
                   with artificial data involve local, distributed,
                   real-valued, and noisy pattern representations. In
                   comparisons with real-time recurrent learning, back
                   propagation through time, recurrent cascade
                   correlation, Elman nets, and neural sequence
                   chunking, LSTM leads to many more successful runs,
                   and learns much faster. LSTM also solves complex,
                   artificial long-time-lag tasks that have never been
                   solved by previous recurrent network algorithms.},
  doi =           {10.1162/neco.1997.9.8.1735},
  issn =          {0899-7667},
  timestamp =     {2020.09.15},
  url =           {https://doi.org/10.1162/neco.1997.9.8.1735},
}

@inproceedings{Cho2014,
  address =       {Doha, Qatar},
  author =        {Cho, Kyunghyun and van Merri{\"e}nboer, Bart and
                   Gulcehre, Caglar and Bahdanau, Dzmitry and
                   Bougares, Fethi and Schwenk, Holger and
                   Bengio, Yoshua},
  booktitle =     {Proceedings of the 2014 Conference on Empirical
                   Methods in Natural Language Processing ({EMNLP})},
  month =         oct,
  pages =         {1724--1734},
  publisher =     {Association for Computational Linguistics},
  title =         {Learning Phrase Representations using {RNN}
                   Encoder{--}Decoder for Statistical Machine
                   Translation},
  year =          {2014},
  doi =           {10.3115/v1/D14-1179},
  timestamp =     {2020.09.15},
  url =           {https://www.aclweb.org/anthology/D14-1179},
}

@article{Kutz2019,
  author =        {Champion, Kathleen and Lusch, Bethany and
                   Kutz, J. Nathan and Brunton, Steven L.},
  journal =       {Proceedings of the National Academy of Sciences},
  number =        {45},
  pages =         {22445--22451},
  publisher =     {National Academy of Sciences},
  title =         {Data-driven discovery of coordinates and governing
                   equations},
  volume =        {116},
  year =          {2019},
  abstract =      {Governing equations are essential to the study of
                   physical systems, providing models that can
                   generalize to predict previously unseen behaviors.
                   There are many systems of interest across disciplines
                   where large quantities of data have been collected,
                   but the underlying governing equations remain
                   unknown. This work introduces an approach to discover
                   governing models from data. The proposed method
                   addresses a key limitation of prior approaches by
                   simultaneously discovering coordinates that admit a
                   parsimonious dynamical model. Developing parsimonious
                   and interpretable governing models has the potential
                   to transform our understanding of complex systems,
                   including in neuroscience, biology, and climate
                   science.The discovery of governing equations from
                   scientific data has the potential to transform
                   data-rich fields that lack well-characterized
                   quantitative descriptions. Advances in sparse
                   regression are currently enabling the tractable
                   identification of both the structure and parameters
                   of a nonlinear dynamical system from data. The
                   resulting models have the fewest terms necessary to
                   describe the dynamics, balancing model complexity
                   with descriptive ability, and thus promoting
                   interpretability and generalizability. This provides
                   an algorithmic approach to Occam{\textquoteright}s
                   razor for model discovery. However, this approach
                   fundamentally relies on an effective coordinate
                   system in which the dynamics have a simple
                   representation. In this work, we design a custom deep
                   autoencoder network to discover a coordinate
                   transformation into a reduced space where the
                   dynamics may be sparsely represented. Thus, we
                   simultaneously learn the governing equations and the
                   associated coordinate system. We demonstrate this
                   approach on several example high-dimensional systems
                   with low-dimensional behavior. The resulting modeling
                   framework combines the strengths of deep neural
                   networks for flexible representation and sparse
                   identification of nonlinear dynamics (SINDy) for
                   parsimonious models. This method places the discovery
                   of coordinates and models on an equal footing.},
  doi =           {10.1073/pnas.1906995116},
  issn =          {0027-8424},
  url =           {https://www.pnas.org/content/116/45/22445},
}

@article{Carlberg2020,
  author =        {Eric J. Parish and Kevin T. Carlberg},
  journal =       {Computer Methods in Applied Mechanics and
                   Engineering},
  pages =         {112990},
  title =         {Time-series machine-learning error models for
                   approximate solutions to parameterized dynamical
                   systems},
  volume =        {365},
  year =          {2020},
  abstract =      {This work proposes a machine-learning framework for
                   modeling the error incurred by approximate solutions
                   to parameterized dynamical systems. In particular, we
                   extend the machine-learning error models (MLEM)
                   framework proposed in Ref. Freno and Carlberg (2019)
                   to dynamical systems. The proposed Time-Series
                   Machine-Learning Error Modeling (T-MLEM) method
                   constructs a regression model that maps features –
                   which comprise error indicators that are derived from
                   standard a posteriori error-quantification techniques
                   – to a random variable for the approximate-solution
                   error at each time instance. The proposed framework
                   considers a wide range of candidate features,
                   regression methods, and additive noise models. We
                   consider primarily recursive regression techniques
                   developed for time-series modeling, including both
                   classical time-series models (e.g., autoregressive
                   models) and recurrent neural networks (RNNs), but
                   also analyze standard non-recursive regression
                   techniques (e.g., feed-forward neural networks) for
                   comparative purposes. Numerical experiments conducted
                   on multiple benchmark problems illustrate that the
                   long short-term memory (LSTM) neural network, which
                   is a type of RNN, outperforms other methods and
                   yields substantial improvements in error predictions
                   over traditional approaches.},
  doi =           {https://doi.org/10.1016/j.cma.2020.112990},
  issn =          {0045-7825},
  timestamp =     {2020.09.22},
  url =           {http://www.sciencedirect.com/science/article/pii/
                  S0045782520301742},
}

@inproceedings{Trask2020,
  author =        {Nathanial Trask and Ravi Patel and Paul Atzberger and
                   Ben Gross},
  booktitle =     {Proceedings of AAAI-MLPS},
  title =         {GMLS-Nets: A machine learning framework for
                   unstructured data},
  year =          {2020},
  abstract =      {Data fields sampled on irregularly spaced points
                   arise in many science and engineering applications.
                   For regular grids, Convolutional Neural Networks
                   (CNNs) gain benefits from weight sharing and
                   invariances. We generalize CNNs by introducing
                   methods for data on unstructured point clouds using
                   Generalized Moving Least Squares (GMLS). GMLS is a
                   nonparametric meshfree technique for estimating
                   linear bounded functionals from scattered data, and
                   has emerged as an effective technique for solving
                   partial differential equations (PDEs). By
                   parameterizing the GMLS estimator, we obtain learning
                   methods for linear and non-linear operators with
                   unstructured stencils. The requisite calculations are
                   local, embarrassingly parallelizable, and supported
                   by a rigorous approximation theory. We show how the
                   framework may be used for unstructured physical data
                   sets to perform operator regression, develop
                   predictive dynamical models, and obtain feature
                   extractors for engineering quantities of interest.
                   The results show the promise of these architectures
                   as foundations for data-driven model development in
                   scientific machine learning applications.},
  timestamp =     {2020.09.03},
  url =           {http://ceur-ws.org/Vol-2587/article_9.pdf},
}

@article{Atzberger2021,
  author =        {Atzberger, Paul J.},
  journal =       {arXiv},
  publisher =     {arXiv},
  title =         {MLMOD Package: Machine Learning Methods for
                   Data-Driven Modeling in LAMMPS},
  year =          {2021},
  doi =           {10.48550/ARXIV.2107.14362},
  url =           {https://arxiv.org/abs/2107.14362},
}

@article{Erichson2019,
  author =        {Erichson, N Benjamin and Muehlebach, Michael and
                   Mahoney, Michael W},
  journal =       {arXiv preprint arXiv:1905.10866},
  title =         {Physics-informed autoencoders for Lyapunov-stable
                   fluid flow prediction},
  year =          {2019},
  timestamp =     {2020.09.15},
}

@article{Bengio2015,
  author =        {Junyoung Chung and Kyle Kastner and Laurent Dinh and
                   Kratarth Goel and Aaron C. Courville and
                   Yoshua Bengio},
  journal =       {Advances in neural information processing systems},
  title =         {A Recurrent Latent Variable Model for Sequential
                   Data},
  volume =        {abs/1506.02216},
  year =          {2015},
  biburl =        {https://dblp.org/rec/journals/corr/ChungKDGCB15.bib},
  bibsource =     {dblp computer science bibliography, https://dblp.org},
  timestamp =     {2020.09.24},
  url =           {http://arxiv.org/abs/1506.02216},
}

@article{Hernandez2018,
  author =        {Hernández, Carlos X. and Wayment-Steele, Hannah K. and
                   Sultan, Mohammad M. and Husic, Brooke E. and
                   Pande, Vijay S.},
  journal =       {Physical Review E},
  month =         {Jun},
  number =        {6},
  publisher =     {American Physical Society (APS)},
  title =         {Variational encoding of complex dynamics},
  volume =        {97},
  year =          {2018},
  doi =           {10.1103/physreve.97.062412},
  issn =          {2470-0053},
  url =           {http://dx.doi.org/10.1103/PhysRevE.97.062412},
}

@inproceedings{Pearce2020,
  author =        {Pearce, Michael},
  editor =        {Cheng Zhang and Francisco Ruiz and Thang Bui and
                   Adji Bousso Dieng and Dawen Liang},
  month =         {08 Dec},
  pages =         {1--12},
  publisher =     {PMLR},
  series =        {Proceedings of Machine Learning Research},
  title =         {The Gaussian Process Prior VAE for Interpretable
                   Latent Dynamics from Pixels},
  volume =        {118},
  year =          {2020},
  abstract =      {We consider the problem of unsupervised learning of a
                   low dimensional, interpretable, latent state of a
                   video containing a moving object. The problem of
                   distilling interpretable dynamics from pixels has
                   been extensively considered through the lens of
                   graphical/state space models (Fraccaro et al., 2017;
                   Lin et al., 2018; Pearce et al., 2018; Chiappa and
                   Paquet, 2019) that exploit Markov structure for cheap
                   computation and structured priors for enforcing
                   interpretability on latent representations. We take a
                   step towards extending these approaches by discarding
                   the Markov structure; inspired by Gaussian process
                   dynamical models (Wang et al., 2006), we instead
                   repurpose the recently proposed Gaussian Process
                   Prior Variational Autoencoder (Casale et al., 2018)
                   for learning interpretable latent dynamics. We
                   describe the model and perform experiments on a
                   synthetic dataset and see that the model reliably
                   reconstructs smooth dynamics exhibiting U-turns and
                   loops. We also observe that this model may be trained
                   without any annealing or freeze-thaw of training
                   parameters in contrast to previous works, albeit for
                   slightly dierent use cases, where application specic
                   training tricks are often required.},
  timestamp =     {2020.09.18},
  url =           {http://proceedings.mlr.press/v118/pearce20a.html},
}

@article{Girin2020,
  author =        {Laurent Girin and Simon Leglaive and Xiaoyu Bie and
                   Julien Diard and Thomas Hueber and
                   Xavier Alameda-Pineda},
  title =         {Dynamical Variational Autoencoders: A Comprehensive
                   Review},
  year =          {2020},
}

@inproceedings{Chen2016,
  author =        {Chen, Nutan and Karl, Maximilian and
                   Van Der Smagt, Patrick},
  booktitle =     {2016 IEEE-RAS 16th International Conference on
                   Humanoid Robots (Humanoids)},
  organization =  {IEEE},
  pages =         {629--636},
  title =         {Dynamic movement primitives in latent space of
                   time-dependent variational autoencoders},
  year =          {2016},
  url =           {https://ieeexplore.ieee.org/document/7803340},
}

@article{Roeder2019,
  author =        {Geoffrey Roeder and Paul K. Grant and Andrew Phillips and
                   Neil Dalchau and Edward Meeds},
  title =         {Efficient Amortised Bayesian Inference for
                   Hierarchical and Nonlinear Dynamical Systems},
  year =          {2019},
  url =           {https://arxiv.org/abs/1905.12090},
}

@article{Kohonen1982,
  author =        {Kohonen, Teuvo},
  journal =       {Biological cybernetics},
  number =        {1},
  pages =         {59--69},
  publisher =     {Springer},
  title =         {Self-organized formation of topologically correct
                   feature maps},
  volume =        {43},
  year =          {1982},
  timestamp =     {2020.09.23},
  url =           {https://link.springer.com/article/10.1007/BF00337288},
}

@inproceedings{Bishop1996,
  author =        {Christopher M. Bishop and Markus Svens{\'{e}}n and
                   Christopher K. I. Williams},
  booktitle =     {Advances in Neural Information Processing Systems 9,
                   NIPS, Denver, CO, USA, December 2-5, 1996},
  editor =        {Michael Mozer and Michael I. Jordan and
                   Thomas Petsche},
  pages =         {354--360},
  publisher =     {{MIT} Press},
  title =         {{GTM:} {A} Principled Alternative to the
                   Self-Organizing Map},
  year =          {1996},
  biburl =        {https://dblp.org/rec/conf/nips/BishopSW96.bib},
  bibsource =     {dblp computer science bibliography, https://dblp.org},
  timestamp =     {2020.09.10},
  url =           {http://papers.nips.cc/paper/1207-gtm-a-principled-
                  alternative-to-the-self-organizing-map},
}

@article{Jensen2020,
  author =        {Kristopher T. Jensen and Ta-Chu Kao and Marco Tripodi and
                   Guillaume Hennequin},
  title =         {Manifold GPLVMs for discovering non-Euclidean latent
                   structure in neural data},
  year =          {2020},
  url =           {https://arxiv.org/abs/2006.07429},
}

@article{Kalatzis2020,
  author =        {{Kalatzis}, Dimitris and {Eklund}, David and
                   {Arvanitidis}, Georgios and {Hauberg}, S{\o}ren},
  journal =       {arXiv e-prints},
  month =         feb,
  pages =         {arXiv:2002.05227},
  title =         {Variational Autoencoders with Riemannian Brownian
                   Motion Priors},
  year =          {2020},
  eid =           {arXiv:2002.05227},
  url =           {https://arxiv.org/abs/2002.05227},
}

@article{Falorsi2018,
  author =        {Luca Falorsi and P. D. Haan and T. Davidson and
                   Nicola De Cao and M. Weiler and Patrick Forr{\'e} and
                   T. Cohen},
  journal =       {ArXiv},
  title =         {Explorations in Homeomorphic Variational
                   Auto-Encoding},
  volume =        {abs/1807.04689},
  year =          {2018},
  url =           {https://arxiv.org/pdf/1807.04689.pdf},
}

@inproceedings{Chen2020,
  address =       {Virtual},
  author =        {Chen, Nutan and Klushyn, Alexej and
                   Ferroni, Francesco and Bayer, Justin and
                   Van Der Smagt, Patrick},
  booktitle =     {Proceedings of the 37th International Conference on
                   Machine Learning},
  editor =        {Hal Daumé III and Aarti Singh},
  month =         {13--18 Jul},
  pages =         {1587--1596},
  publisher =     {PMLR},
  series =        {Proceedings of Machine Learning Research},
  title =         {Learning Flat Latent Manifolds with {VAE}s},
  volume =        {119},
  year =          {2020},
  abstract =      {Measuring the similarity between data points often
                   requires domain knowledge, which can in parts be
                   compensated by relying on unsupervised methods such
                   as latent-variable models, where similarity/distance
                   is estimated in a more compact latent space.
                   Prevalent is the use of the Euclidean metric, which
                   has the drawback of ignoring information about
                   similarity of data stored in the decoder, as captured
                   by the framework of Riemannian geometry. We propose
                   an extension to the framework of variational
                   auto-encoders allows learning flat latent manifolds,
                   where the Euclidean metric is a proxy for the
                   similarity between data points. This is achieved by
                   defining the latent space as a Riemannian manifold
                   and by regularising the metric tensor to be a scaled
                   identity matrix. Additionally, we replace the compact
                   prior typically used in variational auto-encoders
                   with a recently presented, more expressive
                   hierarchical one—and formulate the learning problem
                   as a constrained optimisation problem. We evaluate
                   our method on a range of data-sets, including a
                   video-tracking benchmark, where the performance of
                   our unsupervised approach nears that of
                   state-of-the-art supervised approaches, while
                   retaining the computational efficiency of
                   straight-line-based approaches.},
  url =           {http://proceedings.mlr.press/v119/chen20i.html},
}

@article{Kipf2018,
  author =        {Tim R. Davidson and Luca Falorsi and Nicola De Cao and
                   Thomas Kipf and Jakub M. Tomczak},
  title =         {Hyperspherical Variational Auto-Encoders},
  year =          {2018},
  url =           {https://arxiv.org/abs/1804.00891},
}

@inproceedings{Arvanitidis2018,
  author =        {Georgios Arvanitidis and Lars Kai Hansen and
                   Søren Hauberg},
  booktitle =     {International Conference on Learning Representations},
  title =         {Latent Space Oddity: on the Curvature of Deep
                   Generative Models},
  year =          {2018},
  url =           {https://openreview.net/forum?id=SJzRZ-WCZ},
}

@inproceedings{Rey2020,
  author =        {Perez Rey, Luis A. and Menkovski, Vlado and
                   Portegies, Jim},
  booktitle =     {Proceedings of the Twenty-Ninth International Joint
                   Conference on Artificial Intelligence, IJCAI-20},
  editor =        {Christian Bessiere},
  month =         {7},
  pages =         {2704--2710},
  publisher =     {International Joint Conferences on Artificial
                   Intelligence Organization},
  title =         {Diffusion Variational Autoencoders},
  year =          {2020},
  doi =           {10.24963/ijcai.2020/375},
  url =           {https://arxiv.org/pdf/1901.08991.pdf},
}

@article{Bateman1915,
  author =        {{Bateman}, Harry},
  journal =       {Monthly Weather Review},
  month =         jan,
  number =        {4},
  pages =         {163},
  title =         {{Some Recent Researches on the Motion of Fluids}},
  volume =        {43},
  year =          {1915},
  doi =           {10.1175/1520-0493(1915)43<163:SRROTM>2.0.CO;2},
}

@article{Hopf1950,
  author =        {Hopf, E},
  journal =       {Comm. Pure Appl. Math. 3, 201-230},
  month =         {01},
  title =         {The partial differential equation $u_t + uu_x =
                   \mu_{xx}$},
  year =          {1950},
  timestamp =     {2020.09.10},
  url =           {https://onlinelibrary.wiley.com/doi/abs/10.1002/
                  cpa.3160030302},
}

@book{Killing1885,
  author =        {Killing, Wilhelm},
  publisher =     {BG Teubner},
  title =         {Die nicht-euklidischen Raumformen in analytischer
                   Behandlung},
  year =          {1885},
}

@article{Volkert2013,
  author =        {Volkert, Klaus},
  journal =       {BoMA-Bulletin of the Manifold Atlas},
  title =         {Space forms: a history},
  year =          {2013},
}

@article{LeCun2015,
  author =        {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal =       {Nature},
  month =         may,
  number =        {7553},
  pages =         {436--444},
  title =         {Deep learning},
  volume =        {521},
  year =          {2015},
  abstract =      {Deep learning allows computational models that are
                   composed of multiple processing layers to learn
                   representations of data with multiple levels of
                   abstraction. These methods have dramatically improved
                   the state-of-the-art in speech recognition, visual
                   object recognition, object detection and many other
                   domains such as drug discovery and genomics. Deep
                   learning discovers intricate structure in large data
                   sets by using the backpropagation algorithm to
                   indicate how a machine should change its internal
                   parameters that are used to compute the
                   representation in each layer from the representation
                   in the previous layer. Deep convolutional nets have
                   brought about breakthroughs in processing images,
                   video, speech and audio, whereas recurrent nets have
                   shone light on sequential data such as text and
                   speech.},
  issn =          {1476-4687},
  timestamp =     {2020.09.02},
  url =           {https://doi.org/10.1038/nature14539},
}

@article{Carlsson2014,
  author =        {Perea, Jose A. and Carlsson, Gunnar},
  journal =       {International Journal of Computer Vision},
  month =         mar,
  number =        {1},
  pages =         {75--97},
  title =         {A Klein-Bottle-Based Dictionary for Texture
                   Representation},
  volume =        {107},
  year =          {2014},
  abstract =      {A natural object of study in texture representation
                   and material classification is the probability
                   density function, in pixel-value space, underlying
                   the set of small patches from the given image.
                   Inspired by the fact that small $$n\times
                   n$$n×nhigh-contrast patches from natural images in
                   gray-scale accumulate with high density around a
                   surface $$\fancyscript{K}\subset {\mathbb
                   {R}}^{n^2}$$K⊂Rn2with the topology of a Klein
                   bottle (Carlsson et al. International Journal of
                   Computer Vision 76(1):1-12, 2008), we present in this
                   paper a novel framework for the estimation and
                   representation of distributions around
                   $$\fancyscript{K}$$K, of patches from texture images.
                   More specifically, we show that most $$n\times
                   n$$n×npatches from a given image can be projected
                   onto $$\fancyscript{K}$$Kyielding a finite sample
                   $$S\subset \fancyscript{K}$$S⊂K, whose underlying
                   probability density function can be represented in
                   terms of Fourier-like coefficients, which in turn,
                   can be estimated from $$S$$S. We show that image
                   rotation acts as a linear transformation at the level
                   of the estimated coefficients, and use this to define
                   a multi-scale rotation-invariant descriptor. We test
                   it by classifying the materials in three popular data
                   sets: The CUReT, UIUCTex and KTH-TIPS texture
                   databases.},
  issn =          {1573-1405},
  url =           {https://doi.org/10.1007/s11263-013-0676-2},
}

@article{Sarafianos2016,
  author =        {Nikolaos Sarafianos and Bogdan Boteanu and
                   Bogdan Ionescu and Ioannis A. Kakadiaris},
  journal =       {Computer Vision and Image Understanding},
  pages =         {1 - 20},
  title =         {3D Human pose estimation: A review of the literature
                   and analysis of covariates},
  volume =        {152},
  year =          {2016},
  abstract =      {Estimating the pose of a human in 3D given an image
                   or a video has recently received significant
                   attention from the scientific community. The main
                   reasons for this trend are the ever increasing new
                   range of applications (e.g., human-robot interaction,
                   gaming, sports performance analysis) which are driven
                   by current technological advances. Although recent
                   approaches have dealt with several challenges and
                   have reported remarkable results, 3D pose estimation
                   remains a largely unsolved problem because real-life
                   applications impose several challenges which are not
                   fully addressed by existing methods. For example,
                   estimating the 3D pose of multiple people in an
                   outdoor environment remains a largely unsolved
                   problem. In this paper, we review the recent advances
                   in 3D human pose estimation from RGB images or image
                   sequences. We propose a taxonomy of the approaches
                   based on the input (e.g., single image or video,
                   monocular or multi-view) and in each case we
                   categorize the methods according to their key
                   characteristics. To provide an overview of the
                   current capabilities, we conducted an extensive
                   experimental evaluation of state-of-the-art
                   approaches in a synthetic dataset created
                   specifically for this task, which along with its
                   ground truth is made publicly available for research
                   purposes. Finally, we provide an in-depth discussion
                   of the insights obtained from reviewing the
                   literature and the results of our experiments. Future
                   directions and challenges are identified.},
  doi =           {https://doi.org/10.1016/j.cviu.2016.09.002},
  issn =          {1077-3142},
  url =           {http://www.sciencedirect.com/science/article/pii/
                  S1077314216301369},
}

@Article{Prigogine1967,
  author    = {Prigogine, Ilya and Nicolis, Gr{\'e}goire},
  journal   = {The Journal of Chemical Physics},
  title     = {On symmetry-breaking instabilities in dissipative systems},
  year      = {1967},
  number    = {9},
  pages     = {3542--3550},
  volume    = {46},
  doi       = {10.1063/1.1841255},
  publisher = {American Institute of Physics},
  url       = {https://doi.org/10.1063/1.1841255},
}

@Article{Prigogine1968,
  author    = {Prigogine, Ilya and Lefever, Ren{\'e}},
  journal   = {The Journal of Chemical Physics},
  title     = {Symmetry breaking instabilities in dissipative systems. II},
  year      = {1968},
  number    = {4},
  pages     = {1695--1700},
  volume    = {48},
  doi       = {10.1063/1.1668896},
  publisher = {American Institute of Physics},
  url       = {https://doi.org/10.1063/1.1668896},
}

@Book{Strogatz2018,
  author    = {Strogatz, Steven H},
  publisher = {CRC press},
  title     = {Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering},
  year      = {2018},
  doi       = {10.1201/9780429492563},
  url       = {https://doi.org/10.1201/9780429492563},
}

@book{HirschSmale1974,
  author =        {Hirsch, Morris and Smale, Stephen},
  publisher =     {Academic Press, Inc.},
  title =         {Differential Equations, Dynamical Systems, and Linear
                   Algebra (Pure and Applied Mathematics, Vol. 60)},
  year =          {1974},
}

@Book{HirschSmaleMorris2012,
  author    = {Hirsch, Morris W and Smale, Stephen and Devaney, Robert L},
  publisher = {Academic press},
  title     = {Differential equations, dynamical systems, and an introduction to chaos},
  year      = {2012},
  doi       = {10.1016/C2009-0-61160-0},
  url       = {https://doi.org/10.1016/C2009-0-61160-0},
}

@book{Iserles2009,
  author =        {Iserles, Arieh},
  number =        {44},
  publisher =     {Cambridge university press},
  title =         {A first course in the numerical analysis of
                   differential equations},
  year =          {2009},
}

@book{Burden2010,
  author =        {Richard L. Burden and Douglas Faires},
  publisher =     {Brooks/Cole Cengage Learning},
  title =         {Numerical Analysis},
  year =          {2010},
  timestamp =     {2011.12.09},
}

@article{Zwicker2020,
  author =        {David Zwicker},
  journal =       {Journal of Open Source Software},
  number =        {48},
  pages =         {2158},
  publisher =     {The Open Journal},
  title =         {py-pde: A Python package for solving partial
                   differential equations},
  volume =        {5},
  year =          {2020},
  doi =           {10.21105/joss.02158},
  url =           {https://doi.org/10.21105/joss.02158},
}

@article{Soljacic2020,
  author =        {Lu, Peter Y and Kim, Samuel and
                   Solja{\v{c}}i{\'c}, Marin},
  journal =       {Physical Review X},
  number =        {3},
  pages =         {031056},
  publisher =     {APS},
  title =         {Extracting interpretable physical parameters from
                   spatiotemporal systems using unsupervised learning},
  volume =        {10},
  year =          {2020},
  abstract =      {Experimental data are often affected by uncontrolled
                   variables that make analysis and interpretation
                   difficult. For spatiotemporal systems, this problem
                   is further exacerbated by their intricate dynamics.
                   Modern machine learning methods are particularly well
                   suited for analyzing and modeling complex datasets,
                   but to be effective in science, the result needs to
                   be interpretable. We demonstrate an unsupervised
                   learning technique for extracting interpretable
                   physical parameters from noisy spatiotemporal data
                   and for building a transferable model of the system.
                   In particular, we implement a physics-informed
                   architecture based on variational autoencoders that
                   is designed for analyzing systems governed by partial
                   differential equations. The architecture is trained
                   end to end and extracts latent parameters that
                   parametrize the dynamics of a learned predictive
                   model for the system. To test our method, we train
                   our model on simulated data from a variety of partial
                   differential equations with varying dynamical
                   parameters that act as uncontrolled variables.
                   Numerical experiments show that our method can
                   accurately identify relevant parameters and extract
                   them from raw and even noisy spatiotemporal data
                   (tested with roughly $10\%$ added noise). These
                   extracted parameters correlate well (linearly with
                   $R^2 > 0.95$) with the ground truth physical
                   parameters used to generate the datasets. We then
                   apply this method to nonlinear fiber propagation
                   data, generated by an ab initio simulation, to
                   demonstrate its capabilities on a more realistic
                   dataset. Our method for discovering interpretable
                   latent parameters in spatiotemporal systems will
                   allow us to better analyze and understand real-world
                   phenomena and datasets, which often have unknown and
                   uncontrolled variables that alter the system dynamics
                   and cause varying behaviors that are difficult to
                   disentangle.},
  doi =           {10.1103/PhysRevX.10.031056},
  url =           {https://link.aps.org/doi/10.1103/PhysRevX.10.031056},
}

@Book{Goodfellow2016,
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  publisher = {The MIT Press},
  title     = {Deep Learning},
  year      = {2016},
  isbn      = {0262035618},
  abstract  = {"Written by three experts in the field, Deep Learning
                   is the only comprehensive book on the subject." --
                   Elon Musk, cochair of OpenAI; cofounder and CEO of
                   Tesla and SpaceXDeep learning is a form of machine
                   learning that enables computers to learn from
                   experience and understand the world in terms of a
                   hierarchy of concepts. Because the computer gathers
                   knowledge from experience, there is no need for a
                   human computer operator to formally specify all the
                   knowledge that the computer needs. The hierarchy of
                   concepts allows the computer to learn complicated
                   concepts by building them out of simpler ones; a
                   graph of these hierarchies would be many layers deep.
                   This book introduces a broad range of topics in deep
                   learning. The text offers mathematical and conceptual
                   background, covering relevant concepts in linear
                   algebra, probability theory and information theory,
                   numerical computation, and machine learning. It
                   describes deep learning techniques used by
                   practitioners in industry, including deep feedforward
                   networks, regularization, optimization algorithms,
                   convolutional networks, sequence modeling, and
                   practical methodology; and it surveys such
                   applications as natural language processing, speech
                   recognition, computer vision, online recommendation
                   systems, bioinformatics, and videogames. Finally, the
                   book offers research perspectives, covering such
                   theoretical topics as linear factor models,
                   autoencoders, representation learning, structured
                   probabilistic models, Monte Carlo methods, the
                   partition function, approximate inference, and deep
                   generative models. Deep Learning can be used by
                   undergraduate or graduate students planning careers
                   in either industry or research, and by software
                   engineers who want to begin using deep learning in
                   their products or platforms. A website offers
                   supplementary material for both readers and
                   instructors.},
  timestamp = {2020.09.10},
  url       = {https://www.deeplearningbook.org/},
}

@Article{Atzberger2018,
  author    = {Atzberger, P. J.},
  journal   = {SciML2018 Workshop, position paper},
  title     = {Importance of the Mathematical Foundations of Machine Learning Methods for Scientific and Engineering Applications},
  year      = {2018},
  timestamp = {2020.09.01},
  url       = {https://arxiv.org/abs/1808.02213},
}

@Article{Brunton2016,
  author    = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
  year      = {2016},
  issn      = {0027-8424},
  number    = {15},
  pages     = {3932--3937},
  volume    = {113},
  abstract  = {Understanding dynamic constraints and balances in
                   nature has facilitated rapid development of knowledge
                   and enabled technology, including aircraft,
                   combustion engines, satellites, and electrical power.
                   This work develops a novel framework to discover
                   governing equations underlying a dynamical system
                   simply from data measurements, leveraging advances in
                   sparsity techniques and machine learning. The
                   resulting models are parsimonious, balancing model
                   complexity with descriptive ability while avoiding
                   overfitting. There are many critical data-driven
                   problems, such as understanding cognition from neural
                   recordings, inferring climate patterns, determining
                   stability of financial markets, predicting and
                   suppressing the spread of disease, and controlling
                   turbulence for greener transportation and energy.
                   With abundant data and elusive laws, data-driven
                   discovery of dynamics will continue to play an
                   important role in these efforts.Extracting governing
                   equations from data is a central challenge in many
                   diverse areas of science and engineering. Data are
                   abundant whereas models often remain elusive, as in
                   climate science, neuroscience, ecology, finance, and
                   epidemiology, to name only a few examples. In this
                   work, we combine sparsity-promoting techniques and
                   machine learning with nonlinear dynamical systems to
                   discover governing equations from noisy measurement
                   data. The only assumption about the structure of the
                   model is that there are only a few important terms
                   that govern the dynamics, so that the equations are
                   sparse in the space of possible functions; this
                   assumption holds for many physical systems in an
                   appropriate basis. In particular, we use sparse
                   regression to determine the fewest terms in the
                   dynamic governing equations required to accurately
                   represent the data. This results in parsimonious
                   models that balance accuracy with model complexity to
                   avoid overfitting. We demonstrate the algorithm on a
                   wide range of problems, from simple canonical
                   systems, including linear and nonlinear oscillators
                   and the chaotic Lorenz system, to the fluid vortex
                   shedding behind an obstacle. The fluid example
                   illustrates the ability of this method to discover
                   the underlying dynamics of a system that took experts
                   in the community nearly 30 years to resolve. We also
                   show that this method generalizes to parameterized
                   systems and systems that are time-varying or have
                   external forcing.},
  doi       = {10.1073/pnas.1517384113},
  publisher = {National Academy of Sciences},
  timestamp = {2020.09.10},
  url       = {https://www.pnas.org/content/113/15/3932},
}

@Article{Schmidt2009,
  author    = {Michael Schmidt and Hod Lipson},
  title     = {Distilling Free-Form Natural Laws from Experimental Data},
  year      = {2009},
  issn      = {0036-8075},
  pages     = {81-85},
  volume    = {324},
  doi       = {10.1126/science.1165893},
  timestamp = {2020.09.14},
}

@Article{Atzberger2020,
  author    = {Ryan Lopez and Paul J. Atzberger},
  journal   = {arXiv (preprint)},
  title     = {Variational Autoencoders for Learning Nonlinear Dynamics of Physical Systems},
  year      = {2020},
  doi       = {10.48550/ARXIV.2012.03448},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2012.03448},
}

@InProceedings{Kingma2014,
  author    = {Diederik P. Kingma and Max Welling},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  title     = {Auto-Encoding Variational Bayes},
  year      = {2014},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  url       = {http://arxiv.org/abs/1312.6114},
}

@Article{Blei2017,
  author    = {David M. Blei and Alp Kucukelbir and Jon D. McAuliffe},
  journal   = {Journal of the American Statistical Association},
  title     = {Variational Inference: A Review for Statisticians},
  year      = {2017},
  number    = {518},
  pages     = {859-877},
  volume    = {112},
  abstract  = {One of the core problems of modern statistics
                   is to approximate difficult-to-compute probability
                   densities. This problem is especially important in
                   Bayesian statistics, which frames all inference about
                   unknown quantities as a calculation involving the
                   posterior density. In this article, we review
                   variational inference (VI), a method from machine
                   learning that approximates probability densities
                   through optimization. VI has been used in many
                   applications and tends to be faster than classical
                   methods, such as Markov chain Monte Carlo sampling.
                   The idea behind VI is to first posit a family of
                   densities and then to find a member of that family
                   which is close to the target density. Closeness is
                   measured by Kullback–Leibler divergence. We review
                   the ideas behind mean-field variational inference,
                   discuss the special case of VI applied to exponential
                   family models, present a full example with a Bayesian
                   mixture of Gaussians, and derive a variant that uses
                   stochastic optimization to scale up to massive data.
                   We discuss modern research in VI and highlight
                   important open problems. VI is powerful, but it is
                   not yet well understood. Our hope in writing this
                   article is to catalyze statistical research on this
                   class of algorithms. Supplementary materials for this
                   article are available online.},
  doi       = {10.1080/01621459.2017.1285773},
  publisher = {Taylor \& Francis},
  timestamp = {2020.09.15},
  url       = {https://doi.org/10.1080/01621459.2017.1285773},
}

@Article{Petersen2019,
  author  = {Petersen, Brenden K and Landajuela, Mikel and Mundhenk, T Nathan and Santiago, Claudio P and Kim, Soo K and Kim, Joanne T},
  journal = {arXiv preprint arXiv:1912.04871},
  title   = {Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients},
  year    = {2019},
}

@Article{Landajuela2022,
  author  = {Landajuela, Mikel and Lee, Chak Shing and Yang, Jiachen and Glatt, Ruben and Santiago, Claudio P and Aravena, Ignacio and Mundhenk, Terrell and Mulcahy, Garrett and Petersen, Brenden K},
  journal = {Advances in Neural Information Processing Systems},
  title   = {A Unified Framework for Deep Symbolic Regression},
  year    = {2022},
  pages   = {33985--33998},
  volume  = {35},
}

@Article{Jaafra2019,
  author    = {Jaafra, Yesmina and Laurent, Jean Luc and Deruyver, Aline and Naceur, Mohamed Saber},
  journal   = {Image and Vision Computing},
  title     = {Reinforcement learning for neural architecture search: A review},
  year      = {2019},
  pages     = {57--66},
  volume    = {89},
  publisher = {Elsevier},
}

@InProceedings{Zhang2021,
  author       = {Zhang, Hengzhe and Zhou, Aimin},
  booktitle    = {2021 International Joint Conference on Neural Networks (IJCNN)},
  title        = {RL-GEP: symbolic regression via gene expression programming and reinforcement learning},
  year         = {2021},
  organization = {IEEE},
  pages        = {1--8},
}

@InProceedings{Landajuela2021,
  author       = {Landajuela, Mikel and Petersen, Brenden K and Kim, Sookyung and Santiago, Claudio P and Glatt, Ruben and Mundhenk, Nathan and Pettit, Jacob F and Faissol, Daniel},
  booktitle    = {International Conference on Machine Learning},
  title        = {Discovering symbolic policies with deep reinforcement learning},
  year         = {2021},
  organization = {PMLR},
  pages        = {5979--5989},
}

@Article{Bassenne2019,
  author  = {Bassenne, Maxime and Lozano-Dur{\'a}n, Adri{\'a}n},
  journal = {arXiv preprint arXiv:2001.00008},
  title   = {Computational model discovery with reinforcement learning},
  year    = {2019},
}

@Article{Mundhenk2021,
  author  = {Mundhenk, Terrell and Landajuela, Mikel and Glatt, Ruben and Santiago, Claudio P and Petersen, Brenden K and others},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Symbolic regression via deep reinforcement learning enhanced genetic programming seeding},
  year    = {2021},
  pages   = {24912--24923},
  volume  = {34},
}

@InProceedings{Verma2018,
  author       = {Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
  booktitle    = {International Conference on Machine Learning},
  title        = {Programmatically interpretable reinforcement learning},
  year         = {2018},
  organization = {PMLR},
  pages        = {5045--5054},
}

@Comment{jabref-meta: databaseType:bibtex;}
